{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nFEa9kS9LNWp"
      },
      "source": [
        "# Assignment 3\n",
        "\n",
        "## Instructions\n",
        "- Run this notebook on ```Google Colab(preferable)```\n",
        "- Write your code and analysis in the indicated cells.\n",
        "- Ensure that this notebook runs without errors when the cells are run in sequence.\n",
        "- Do not attempt to change the contents of other cells. \n",
        "\n",
        "## Packages Used\n",
        "- sklearn [link](https://scikit-learn.org/)\n",
        "- Keras [link](https://keras.io/guides/)\n",
        "\n",
        "## Submission\n",
        "- Rename the notebook to `<roll_number>_Assignment3_Q3.ipynb`.\n",
        "\n",
        "\n",
        "## Question 3\n",
        "Fake news is a widespread problem and there are many methods for combating it.\n",
        "You have to build a fake news detection system using a ML model. Train any ML model (ANN, LSTM) over the given Dataset.\n",
        "The dataset has short statements spoken by people and has the meta-information and corresponding label for those sentences. \n",
        "Your target is label column which has 6 labels(in the increasing order of truthfullness): pants-fire, false, barely-true, half-true, mostly-true, true.\n",
        "\n",
        "The features are 'statement', 'subject', 'speaker', 'job', 'state', 'party', 'barely_true_c', 'false_c', 'half_true_c', 'mostly_true_c', 'pants_on_fire_c', 'venue' and the target is column \"label\".\n",
        "\n",
        "The statement is made by speaker whose job, party are given along with 6 columns which are an account of the  type of news(labels) the person has shared before. \n",
        "The person who has shared fake content before is likely to share it in future and this can be accounted by the ML model as a feature. Column barely_true_c contains how many barely_true news has the speaker shared (and so is with column X_c, value of X_c is number of X the person shared).\n",
        "\n",
        "\n",
        "You have to perform two tasks:\n",
        "* task1: Binary classification <br>\n",
        "Classify the given news as true/false. Take the labels pants-fire, false, barely-true as false and rest (half-true, mostly-true, true) as true.\n",
        "* task2: Six-way classification <br>\n",
        "Classify the given news into six-classes \"pants-fire, false, barely-true, half-true, mostly-true, true\".\n",
        "\n",
        "For each of the tasks:\n",
        "1) Experiment with depth of network and try to fine-tune hyperparameters reporting your observations. <br>\n",
        "2) Report the accuracy, f1-score, confusion matrix on train, val and test sets. <br>\n",
        "3) Experiment with bag-of-words, glove and bert embeddings(code given in the below notebook) and report results. <br> Comment on what is the affect of embedding on the results.\n",
        "\n",
        "The pre-processing code is provided, you need to write the training and test.\n",
        "\n",
        "Note: You are supposed to train on trainset, fine-tune on val and just eval on test set. If found that you trained on val/test sets, the penalty will be incurred."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "YYDKsox6LNWx"
      },
      "outputs": [],
      "source": [
        "# !pip install numpy\n",
        "# !pip install tensorflow\n",
        "# !pip install re\n",
        "# !pip install nltk\n",
        "# !pip install keras\n",
        "# !pip install sklearn"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Agl6JEo_gaBT",
        "outputId": "04cd5839-68e3-4771-bd5b-d04807964fd1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        }
      ],
      "source": [
        "# Importing libraries\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from tensorflow import keras  #feel free to use any other library\n",
        "import numpy as np\n",
        "\n",
        "import re\n",
        "import nltk\n",
        "import numpy as np\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem.porter import PorterStemmer\n",
        "nltk.download('stopwords')\n",
        "from sklearn.preprocessing import LabelEncoder, OneHotEncoder\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from keras.utils import np_utils\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from sklearn.metrics import accuracy_score, f1_score\n",
        "\n",
        "import keras\n",
        "from keras.models import Sequential,Model\n",
        "from keras.layers import CuDNNLSTM, Dense, Dropout, LSTM,Input\n",
        "from keras.optimizers import adam_v2\n",
        "from keras.layers.merge import concatenate"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "3g16lblbLNW5"
      },
      "outputs": [],
      "source": [
        "train = pd.read_csv('/content/sample_data/train.csv')\n",
        "val = pd.read_csv('/content/sample_data/val.csv')\n",
        "test = pd.read_csv('/content/sample_data/test.csv')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "enAZ4DvUffVr"
      },
      "outputs": [],
      "source": [
        "# Dropping the 'id' column\n",
        "train.drop('id', axis = 1, inplace = True)\n",
        "test.drop('id', axis = 1, inplace = True)\n",
        "val.drop('id', axis = 1, inplace = True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 687
        },
        "id": "7pEJ-G4yITrd",
        "outputId": "938c9f40-1085-416d-9f9a-c8e8865bb059"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "\n",
              "  <div id=\"df-df265729-62a9-4cd0-83f8-7eba23a819fc\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>label</th>\n",
              "      <th>statement</th>\n",
              "      <th>subject</th>\n",
              "      <th>speaker</th>\n",
              "      <th>job</th>\n",
              "      <th>state</th>\n",
              "      <th>party</th>\n",
              "      <th>barely_true_c</th>\n",
              "      <th>false_c</th>\n",
              "      <th>half_true_c</th>\n",
              "      <th>mostly_true_c</th>\n",
              "      <th>pants_on_fire_c</th>\n",
              "      <th>venue</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>False</td>\n",
              "      <td>Says the Annies List political group supports ...</td>\n",
              "      <td>abortion</td>\n",
              "      <td>dwayne-bohac</td>\n",
              "      <td>State representative</td>\n",
              "      <td>Texas</td>\n",
              "      <td>republican</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>a mailer</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>half-true</td>\n",
              "      <td>When did the decline of coal start? It started...</td>\n",
              "      <td>energy,history,job-accomplishments</td>\n",
              "      <td>scott-surovell</td>\n",
              "      <td>State delegate</td>\n",
              "      <td>Virginia</td>\n",
              "      <td>democrat</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>a floor speech.</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>mostly-true</td>\n",
              "      <td>Hillary Clinton agrees with John McCain \"by vo...</td>\n",
              "      <td>foreign-policy</td>\n",
              "      <td>barack-obama</td>\n",
              "      <td>President</td>\n",
              "      <td>Illinois</td>\n",
              "      <td>democrat</td>\n",
              "      <td>70</td>\n",
              "      <td>71</td>\n",
              "      <td>160</td>\n",
              "      <td>163</td>\n",
              "      <td>9</td>\n",
              "      <td>Denver</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>False</td>\n",
              "      <td>Health care reform legislation is likely to ma...</td>\n",
              "      <td>health-care</td>\n",
              "      <td>blog-posting</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>none</td>\n",
              "      <td>7</td>\n",
              "      <td>19</td>\n",
              "      <td>3</td>\n",
              "      <td>5</td>\n",
              "      <td>44</td>\n",
              "      <td>a news release</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>half-true</td>\n",
              "      <td>The economic turnaround started at the end of ...</td>\n",
              "      <td>economy,jobs</td>\n",
              "      <td>charlie-crist</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Florida</td>\n",
              "      <td>democrat</td>\n",
              "      <td>15</td>\n",
              "      <td>9</td>\n",
              "      <td>20</td>\n",
              "      <td>19</td>\n",
              "      <td>2</td>\n",
              "      <td>an interview on CNN</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-df265729-62a9-4cd0-83f8-7eba23a819fc')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-df265729-62a9-4cd0-83f8-7eba23a819fc button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-df265729-62a9-4cd0-83f8-7eba23a819fc');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ],
            "text/plain": [
              "         label                                          statement  \\\n",
              "0        False  Says the Annies List political group supports ...   \n",
              "1    half-true  When did the decline of coal start? It started...   \n",
              "2  mostly-true  Hillary Clinton agrees with John McCain \"by vo...   \n",
              "3        False  Health care reform legislation is likely to ma...   \n",
              "4    half-true  The economic turnaround started at the end of ...   \n",
              "\n",
              "                              subject         speaker                   job  \\\n",
              "0                            abortion    dwayne-bohac  State representative   \n",
              "1  energy,history,job-accomplishments  scott-surovell        State delegate   \n",
              "2                      foreign-policy    barack-obama             President   \n",
              "3                         health-care    blog-posting                   NaN   \n",
              "4                        economy,jobs   charlie-crist                   NaN   \n",
              "\n",
              "      state       party  barely_true_c  false_c  half_true_c  mostly_true_c  \\\n",
              "0     Texas  republican              0        1            0              0   \n",
              "1  Virginia    democrat              0        0            1              1   \n",
              "2  Illinois    democrat             70       71          160            163   \n",
              "3       NaN        none              7       19            3              5   \n",
              "4   Florida    democrat             15        9           20             19   \n",
              "\n",
              "   pants_on_fire_c                venue  \n",
              "0                0             a mailer  \n",
              "1                0      a floor speech.  \n",
              "2                9               Denver  \n",
              "3               44       a news release  \n",
              "4                2  an interview on CNN  "
            ]
          },
          "execution_count": 5,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "train.head(5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UbFqDO8_U6df",
        "outputId": "fd31e21b-11b2-4887-dc72-71e0fb25a326"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(10269, 13)\n",
            "(1284, 13)\n",
            "(1283, 13)\n"
          ]
        }
      ],
      "source": [
        "# Checking the shape of data\n",
        "print(train.shape)\n",
        "print(val.shape)\n",
        "print(test.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g36lAg-2LNW-"
      },
      "source": [
        "## Clean and pre-process data\n",
        "* Replace missing values\n",
        "* Remove numbers and special characters\n",
        "* Convert to upper-case\n",
        "\n",
        "We experiment with two types of processing, one directly appending the other attributes like subject, job, state, party to sentence and then applying bag of words on it.\n",
        "\n",
        "Other being encoding sentence with glove embeddings and passing just that."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "w7tTpAClApgJ"
      },
      "outputs": [],
      "source": [
        "\n",
        "def dataPreprocessing(data):\n",
        "    '''Function for cleaning the dataset\n",
        "    '''\n",
        "    corpus = []\n",
        "    # Missing values\n",
        "    data[\"job\"].fillna(\"no-job\", inplace = True)\n",
        "    data[\"state\"].fillna(\"no-state\", inplace = True)\n",
        "\n",
        "    for x in range(data.shape[0]):\n",
        "        statement = re.sub('[^a-zA-Z]', ' ', data['statement'][x]) # Removing all numbers and special characters\n",
        "        statement = statement.lower() # Converting uppercase to lowercase\n",
        "        statement = statement.split()\n",
        "        \n",
        "        # you can experiment with any other stemmers\n",
        "        ps = PorterStemmer()\n",
        "        statement = [ps.stem(word) for word in statement if not word in set(stopwords.words('english'))] # Stemming the dataset and removing stopwords\n",
        "        statement = ' '.join(statement)\n",
        "        subject = data['subject'][x].replace(',', ' ')\n",
        "        speaker = data['speaker'][x]\n",
        "        job = data['job'][x].lower()\n",
        "        # job = job.replace(' ', '-')\n",
        "        state = data['state'][x].lower()\n",
        "        party = data['party'][x].lower()\n",
        "        corpus.append(statement + ' '  + subject + ' ' + job + ' ' + state + ' ' + party)\n",
        "    return corpus"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "uy1ikPhJ9LoS"
      },
      "outputs": [],
      "source": [
        "x_train = dataPreprocessing(train)\n",
        "x_val = dataPreprocessing(val) \n",
        "x_test = dataPreprocessing(test) "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2duJ0pJYLNXE",
        "outputId": "aca2e7b7-e8a6-4ff0-c493-661200fe4aaa"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(10269, 1284, 1283)"
            ]
          },
          "execution_count": 9,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "len(x_train), len(x_val), len(x_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "mApDHS1MLNXF"
      },
      "outputs": [],
      "source": [
        "corpus = x_train + x_val + x_test"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-aEEt__-LNXG"
      },
      "source": [
        "## Using bag-of-words embedding\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "sopw2zusZwn4"
      },
      "outputs": [],
      "source": [
        "# Converting the corpus into bag-of-words\n",
        "cv = CountVectorizer(max_features = 8000)\n",
        "X = cv.fit_transform(corpus).toarray()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lWFagD7YLNXI",
        "outputId": "9cbad453-e4af-4cdc-b1e0-151c2de7e217"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "array([[0, 0, 0, ..., 0, 0, 0],\n",
              "       [0, 0, 0, ..., 0, 0, 0],\n",
              "       [0, 0, 0, ..., 0, 0, 0],\n",
              "       ...,\n",
              "       [0, 0, 0, ..., 0, 0, 0],\n",
              "       [1, 0, 0, ..., 0, 0, 0],\n",
              "       [0, 0, 0, ..., 0, 0, 0]])"
            ]
          },
          "execution_count": 12,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "X"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "o06bP9FJEaMU",
        "outputId": "c59f3abc-b16d-42c0-e6e2-e78c0a4fe850"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(12836, 8000)"
            ]
          },
          "execution_count": 13,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "X.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bWQpos4zLNXK",
        "outputId": "d54ccdb2-e1ce-4007-d0a6-418fad47d88d"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "Index(['label', 'statement', 'subject', 'speaker', 'job', 'state', 'party',\n",
              "       'barely_true_c', 'false_c', 'half_true_c', 'mostly_true_c',\n",
              "       'pants_on_fire_c', 'venue'],\n",
              "      dtype='object')"
            ]
          },
          "execution_count": 14,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "train.columns"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "RCqMgDpiLDhu"
      },
      "outputs": [],
      "source": [
        "# Selecting the columns 'barely_true_c',\t'false_c',\t'half_true_c',\t'mostly_true_c',\t'pants_on_fire_c'\n",
        "label_cols = ['barely_true_c', 'false_c', 'half_true_c', 'mostly_true_c',\n",
        "       'pants_on_fire_c']\n",
        "x_train2 = train[label_cols]\n",
        "x_val2 = val[label_cols]\n",
        "x_test2 = test[label_cols]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_J6o-X3pUcXA",
        "outputId": "73fbb76a-dca6-44ea-e49c-5c83715da92f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "      barely_true_c  false_c  half_true_c  mostly_true_c  pants_on_fire_c\n",
            "0                 1        0            1              0                0\n",
            "1                11       43            8              5              105\n",
            "2                 0        1            1              1                0\n",
            "3                 0        1            1              1                0\n",
            "4                70       71          160            163                9\n",
            "...             ...      ...          ...            ...              ...\n",
            "1279             70       71          160            163                9\n",
            "1280             40       29           69             76                7\n",
            "1281              0        1            0              2                0\n",
            "1282              1        4            4              1                0\n",
            "1283              9       11           10              7                3\n",
            "\n",
            "[1284 rows x 5 columns]\n",
            "[[0 0 0 ... 0 0 0]\n",
            " [0 0 0 ... 0 0 0]\n",
            " [0 0 0 ... 0 0 0]\n",
            " ...\n",
            " [0 0 0 ... 0 0 0]\n",
            " [0 0 0 ... 0 0 0]\n",
            " [0 0 0 ... 0 0 0]]\n"
          ]
        }
      ],
      "source": [
        "print(x_val2)\n",
        "print(X[:len(x_train)])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "QglKXzA_w6DH"
      },
      "outputs": [],
      "source": [
        "# Stacking x_train and x_train2 horizontally\n",
        "x_train_bow = np.hstack((X[:len(x_train)], x_train2))\n",
        "x_val_bow = np.hstack((X[len(x_train):len(x_train)+len(x_val)], x_val2))\n",
        "x_test_bow = np.hstack((X[len(x_train)+len(x_val):], x_test2))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "E3pskgViw99U",
        "outputId": "6325f7f1-ca62-45a7-efbb-72ee9f7db032"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(10269, 8005)"
            ]
          },
          "execution_count": 18,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "x_train_bow.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ib8aX82fLNXO"
      },
      "source": [
        "## Use of Glove Embedding\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4W-Pi-l0LNXO"
      },
      "source": [
        "download glove embeddings from 'https://nlp.stanford.edu/data/glove.6B.zip','glove.6B.zip'\n",
        "and place in your current working folder\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dGaW2hKfLNXP",
        "outputId": "2225eff1-4471-4740-9000-72a223dccd13"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "--2022-04-13 17:15:58--  https://nlp.stanford.edu/data/glove.6B.zip\n",
            "Resolving nlp.stanford.edu (nlp.stanford.edu)... 171.64.67.140\n",
            "Connecting to nlp.stanford.edu (nlp.stanford.edu)|171.64.67.140|:443... connected.\n",
            "HTTP request sent, awaiting response... 301 Moved Permanently\n",
            "Location: http://downloads.cs.stanford.edu/nlp/data/glove.6B.zip [following]\n",
            "--2022-04-13 17:15:58--  http://downloads.cs.stanford.edu/nlp/data/glove.6B.zip\n",
            "Resolving downloads.cs.stanford.edu (downloads.cs.stanford.edu)... 171.64.64.22\n",
            "Connecting to downloads.cs.stanford.edu (downloads.cs.stanford.edu)|171.64.64.22|:80... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 862182613 (822M) [application/zip]\n",
            "Saving to: ‘glove.6B.zip.2’\n",
            "\n",
            "glove.6B.zip.2      100%[===================>] 822.24M  5.09MB/s    in 2m 40s  \n",
            "\n",
            "2022-04-13 17:18:38 (5.14 MB/s) - ‘glove.6B.zip.2’ saved [862182613/862182613]\n",
            "\n",
            "Archive:  glove.6B.zip\n",
            "replace glove/glove.6B.50d.txt? [y]es, [n]o, [A]ll, [N]one, [r]ename: n\n",
            "replace glove/glove.6B.100d.txt? [y]es, [n]o, [A]ll, [N]one, [r]ename: n\n",
            "replace glove/glove.6B.200d.txt? [y]es, [n]o, [A]ll, [N]one, [r]ename: n\n",
            "replace glove/glove.6B.300d.txt? [y]es, [n]o, [A]ll, [N]one, [r]ename: n\n"
          ]
        }
      ],
      "source": [
        "!wget https://nlp.stanford.edu/data/glove.6B.zip\n",
        "!unzip \"glove.6B.zip\" -d \"glove\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "o9HKzmGfLNXP"
      },
      "outputs": [],
      "source": [
        "emmbed_dict = {}\n",
        "with open('glove/glove.6B.200d.txt','r') as f:\n",
        "  for line in f:\n",
        "    values = line.split()\n",
        "    word = values[0]\n",
        "    vector = np.asarray(values[1:],'float32')\n",
        "    emmbed_dict[word]=vector\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "4RSeKWT5LNXQ"
      },
      "outputs": [],
      "source": [
        "emmbed_dict['oov'] = np.zeros(200)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "H9vczXnnLNXQ",
        "outputId": "5eee3cdb-dac7-4a22-b911-72a33df23f6e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: sentence-transformers in /usr/local/lib/python3.7/dist-packages (2.2.0)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.7/dist-packages (from sentence-transformers) (1.0.2)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (from sentence-transformers) (1.4.1)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from sentence-transformers) (4.63.0)\n",
            "Requirement already satisfied: transformers<5.0.0,>=4.6.0 in /usr/local/lib/python3.7/dist-packages (from sentence-transformers) (4.18.0)\n",
            "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.7/dist-packages (from sentence-transformers) (0.1.96)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from sentence-transformers) (1.21.5)\n",
            "Requirement already satisfied: huggingface-hub in /usr/local/lib/python3.7/dist-packages (from sentence-transformers) (0.5.1)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.7/dist-packages (from sentence-transformers) (3.2.5)\n",
            "Requirement already satisfied: torch>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from sentence-transformers) (1.10.0+cu111)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.7/dist-packages (from sentence-transformers) (0.11.1+cu111)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch>=1.6.0->sentence-transformers) (3.10.0.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers<5.0.0,>=4.6.0->sentence-transformers) (2.23.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.7/dist-packages (from transformers<5.0.0,>=4.6.0->sentence-transformers) (6.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers<5.0.0,>=4.6.0->sentence-transformers) (2019.12.20)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers<5.0.0,>=4.6.0->sentence-transformers) (3.6.0)\n",
            "Requirement already satisfied: sacremoses in /usr/local/lib/python3.7/dist-packages (from transformers<5.0.0,>=4.6.0->sentence-transformers) (0.0.49)\n",
            "Requirement already satisfied: tokenizers!=0.11.3,<0.13,>=0.11.1 in /usr/local/lib/python3.7/dist-packages (from transformers<5.0.0,>=4.6.0->sentence-transformers) (0.12.1)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from transformers<5.0.0,>=4.6.0->sentence-transformers) (21.3)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from transformers<5.0.0,>=4.6.0->sentence-transformers) (4.11.3)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->transformers<5.0.0,>=4.6.0->sentence-transformers) (3.0.7)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->transformers<5.0.0,>=4.6.0->sentence-transformers) (3.7.0)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from nltk->sentence-transformers) (1.15.0)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers<5.0.0,>=4.6.0->sentence-transformers) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers<5.0.0,>=4.6.0->sentence-transformers) (2021.10.8)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers<5.0.0,>=4.6.0->sentence-transformers) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers<5.0.0,>=4.6.0->sentence-transformers) (1.24.3)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers<5.0.0,>=4.6.0->sentence-transformers) (7.1.2)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers<5.0.0,>=4.6.0->sentence-transformers) (1.1.0)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn->sentence-transformers) (3.1.0)\n",
            "Requirement already satisfied: pillow!=8.3.0,>=5.3.0 in /usr/local/lib/python3.7/dist-packages (from torchvision->sentence-transformers) (7.1.2)\n"
          ]
        }
      ],
      "source": [
        "!pip install sentence-transformers\n",
        "from sentence_transformers import SentenceTransformer\n",
        "model = SentenceTransformer('paraphrase-MiniLM-L6-v2')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EGMxzRDzLNXR",
        "outputId": "8e7ef626-3a04-480e-b70c-ca9b34a7b4d9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        }
      ],
      "source": [
        "from nltk.tokenize import word_tokenize\n",
        "nltk.download('punkt')\n",
        "def dataPreprocessing_glove(data):\n",
        "    corpus = []\n",
        "    # Missing values\n",
        "    data[\"job\"].fillna(\"no-job\", inplace = True)\n",
        "    data[\"state\"].fillna(\"no-state\", inplace = True)\n",
        "\n",
        "    for x in range(data.shape[0]):\n",
        "        statement = re.sub('[^a-zA-Z]', ' ', data['statement'][x]) # Removing all numbers and special characters\n",
        "        statement = statement.lower() # Converting uppercase to lowercase\n",
        "        statement = word_tokenize(statement)\n",
        "\n",
        "        embed_statement = []\n",
        "        for w in statement:\n",
        "            if w in emmbed_dict:\n",
        "                embed_statement.append(emmbed_dict[w])\n",
        "            else:\n",
        "                embed_statement.append(emmbed_dict['oov'])\n",
        "         \n",
        "        # bonus: Think how you can encode the below features(hint: look upon label encoding or training your own word2vec or any other embedding model)\n",
        "    \n",
        "#         subject = data['subject'][x].replace(',', ' ')\n",
        "#         speaker = data['speaker'][x]\n",
        "#         job = data['job'][x].lower()\n",
        "#         # job = job.replace(' ', '-')\n",
        "#         state = data['state'][x].lower()\n",
        "#         party = data['party'][x].lower()\n",
        "        corpus.append(embed_statement)\n",
        "    \n",
        "    return np.array(corpus)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RqGDZUE2LNXS",
        "outputId": "4c54651f-ea9e-4bd1-c8c4-a7979af801f3"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:31: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n"
          ]
        }
      ],
      "source": [
        "x_train_glove = dataPreprocessing_glove(train)\n",
        "x_val_glove = dataPreprocessing_glove(val) \n",
        "x_test_glove = dataPreprocessing_glove(test) "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "u3DOoTn8LNXT"
      },
      "outputs": [],
      "source": [
        "x_train_glove = np.hstack((x_train_glove.reshape(-1,1), x_train2))\n",
        "x_val_glove = np.hstack((x_val_glove.reshape(-1,1), x_val2))\n",
        "x_test_glove = np.hstack((x_test_glove.reshape(-1,1), x_test2))\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "18IXcC7dhJsJ",
        "outputId": "5f898e89-3ef3-4e56-c29a-eec6386cb7f3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(10269, 6)\n",
            "12\n"
          ]
        }
      ],
      "source": [
        "print(x_train_glove.shape)\n",
        "print(len(x_train_glove[0][0]))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lpbN1XaaLNXU"
      },
      "source": [
        "## Use of bert embeddings\n",
        "note: we used our pre-processed code for bow which has the attributed appended to end the end of sentence. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "id": "Es_vKaYaLNXU"
      },
      "outputs": [],
      "source": [
        "from sentence_transformers import SentenceTransformer\n",
        "model = SentenceTransformer('paraphrase-MiniLM-L6-v2')\n",
        "\n",
        "x_train_bert = np.hstack((model.encode(x_train), x_train2))\n",
        "x_val_bert = np.hstack((model.encode(x_val), x_val2))\n",
        "x_test_bert = np.hstack((model.encode(x_test), x_test2))\n",
        "\n",
        "#x_train_bert = model.encode(x_train)\n",
        "#x_val_bert = model.encode(x_val)\n",
        "#x_test_bert = model.encode(x_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SLwIhYiSZJfW",
        "outputId": "5c2af625-5dbc-4af3-80f6-9e9253dc5e02"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(10269, 8005)\n",
            "(10269, 389)\n",
            "(10269, 6)\n",
            "<class 'numpy.ndarray'>\n",
            "200 200 200\n"
          ]
        }
      ],
      "source": [
        "print(x_train_bow.shape)\n",
        "print(x_train_bert.shape)\n",
        "print(x_train_glove.shape)\n",
        "print(type(x_train_bow))\n",
        "print(len(x_train_glove[0][0][1]),len(x_train_glove[1][0][0]),len(x_train_glove[2][0][1]))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pNIlpIfnLNXV"
      },
      "source": [
        "Now use the above 3 types of embedded inputs(bow, glove, bert embeddings) for the 2 classification tasks and compare their outputs\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lmavEzWHrTC8"
      },
      "source": [
        "# Six-way classification"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1wAhr39Aq41J"
      },
      "source": [
        "## Preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "id": "yJwZwMXANg9_"
      },
      "outputs": [],
      "source": [
        "num_classes = 6\n",
        "# Preprocessing function for the labels\n",
        "def categorize(data):\n",
        "    y = data[\"label\"].tolist()\n",
        "\n",
        "    # Encoding the Dependent Variable\n",
        "    labelencoder_y = LabelEncoder()\n",
        "    y = labelencoder_y.fit_transform(y)\n",
        "\n",
        "    # Converting to binary class matrix\n",
        "    y = np_utils.to_categorical(y, num_classes)\n",
        "    return y"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "id": "XIKTUSM3MJ-u"
      },
      "outputs": [],
      "source": [
        "y_train_six_way = categorize(train)\n",
        "y_test_six_way = categorize(test)\n",
        "y_val_six_way = categorize(val)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DGvcZbeAZ0w6",
        "outputId": "4f83df4f-c59b-4d81-adb4-7619ca4316da"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(10269, 6)\n"
          ]
        }
      ],
      "source": [
        "print(y_train_six_way.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cvb1hhgeLNXY"
      },
      "source": [
        "Build a model and pass bow, glove and bert embedded inputs: x_train_bow, x_train_glove, x_train_bert(similarly validate for val and report results on test)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2Y-dusAUolnI"
      },
      "source": [
        "## Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "id": "SaGb3nBiLNXY"
      },
      "outputs": [],
      "source": [
        "## write your code here\n",
        "\n",
        "# Initialize hyperparameters\n",
        "batch_size=32\n",
        "units=128\n",
        "drop_out=0.1\n",
        "learning_rate = 0.01\n",
        "decay=1e-6\n",
        "epoch=5\n",
        "embedding_size_bag=8005\n",
        "embedding_size_bert=389\n",
        "\n",
        "# Create model\n",
        "\"\"\"\n",
        "visible1 = Input(shape=(embedding_size,1)) # for each batch size\n",
        "lstmLayer = LSTM(units=units)(visible1)\n",
        "visible2 = Input(shape=(5,))\n",
        "merge = concatenate([visible2, lstmLayer])\n",
        "#merge1=Dense(32, activation='relu')(merge)\n",
        "output = Dense(6, activation='softmax')(merge)\n",
        "model = Model(inputs=[visible1, visible2], outputs=output)\n",
        "print(model.summary())\n",
        "\"\"\"\n",
        "optimizer = adam_v2.Adam(learning_rate=learning_rate,decay=decay)\n",
        "\n",
        "# Simple Nueral net model\n",
        "def models(embedding_size,x_train, y_train_six_way1,x_val,sixclass=True, val1=[]):\n",
        "  NNmodel = Sequential()\n",
        "  NNmodel.add(Dense(1024, input_dim=embedding_size, activation='relu'))\n",
        "  NNmodel.add(Dense(256, activation='relu'))\n",
        "  NNmodel.add(Dense(126, activation='relu'))\n",
        "  NNmodel.add(Dense(64, activation='relu'))\n",
        "  if sixclass:\n",
        "    NNmodel.add(Dense(6, activation='softmax'))\n",
        "  else:\n",
        "    NNmodel.add(Dense(2, activation='softmax'))\n",
        "  print(NNmodel.summary())\n",
        "\n",
        "  NNmodel.compile(optimizer=optimizer, loss='categorical_crossentropy',metrics=['accuracy'])\n",
        "  # train\n",
        "  if sixclass:\n",
        "    NNmodel.fit(x_train, y_train_six_way1, epochs=epoch, batch_size=batch_size, shuffle=True,validation_data=(x_val,y_val_six_way))\n",
        "  else:\n",
        "    NNmodel.fit(x_train, y_train_six_way1, epochs=epoch, batch_size=batch_size, shuffle=True,validation_data=(x_val,val1))\n",
        "  return NNmodel\n",
        "\n",
        "\"\"\"\n",
        "categorical_crossentropy (cce) produces a one-hot array containing the probable match for each category,\n",
        "sparse_categorical_crossentropy (scce) produces a category index of the most likely matching category.\n",
        "\"\"\"\n",
        "\n",
        "# test\n",
        "# report accuracy, f1-score and confusion matrix\n",
        "\n",
        "def confusion_utility(x_test,y_train_six_way,NNmodel):\n",
        "  y_test=[]\n",
        "  y_pred=[]\n",
        "  for i in range(len(x_test)):\n",
        "    pred = NNmodel.predict(x_test[i:i+1])\n",
        "    a=int(np.argmax(pred[0]))\n",
        "    b=int(np.argmax(y_train_six_way[i]))\n",
        "    y_pred.append(a)\n",
        "    y_test.append(b)\n",
        "  return y_test,y_pred\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MXBcgigNNtfY"
      },
      "source": [
        "**Bag of word embedding on 6 way classification**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "56E7yseLNybn",
        "outputId": "b7f07442-5047-4f30-b62d-bd0a436ddbfc"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " dense (Dense)               (None, 1024)              8198144   \n",
            "                                                                 \n",
            " dense_1 (Dense)             (None, 256)               262400    \n",
            "                                                                 \n",
            " dense_2 (Dense)             (None, 126)               32382     \n",
            "                                                                 \n",
            " dense_3 (Dense)             (None, 64)                8128      \n",
            "                                                                 \n",
            " dense_4 (Dense)             (None, 6)                 390       \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 8,501,444\n",
            "Trainable params: 8,501,444\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n",
            "Epoch 1/5\n",
            "321/321 [==============================] - 8s 17ms/step - loss: 1.6931 - accuracy: 0.3063 - val_loss: 1.5420 - val_accuracy: 0.3785\n",
            "Epoch 2/5\n",
            "321/321 [==============================] - 4s 13ms/step - loss: 1.3827 - accuracy: 0.4317 - val_loss: 1.4990 - val_accuracy: 0.3738\n",
            "Epoch 3/5\n",
            "321/321 [==============================] - 5s 16ms/step - loss: 1.1660 - accuracy: 0.5223 - val_loss: 1.6735 - val_accuracy: 0.3847\n",
            "Epoch 4/5\n",
            "321/321 [==============================] - 4s 11ms/step - loss: 1.0090 - accuracy: 0.5833 - val_loss: 2.0150 - val_accuracy: 0.3777\n",
            "Epoch 5/5\n",
            "321/321 [==============================] - 3s 10ms/step - loss: 0.9345 - accuracy: 0.6234 - val_loss: 2.0017 - val_accuracy: 0.3598\n",
            "Confusion Matrix\n",
            "\n",
            "[[120  23  22  65   9  11]\n",
            " [ 35  46  17  76  34   3]\n",
            " [ 55  16  56  70  12   5]\n",
            " [ 45  27  35 138  20   2]\n",
            " [ 38  32  16 104  59   0]\n",
            " [ 38   2   8   9   5  30]]\n",
            "\n",
            "Accuracy:  0.34996102883865937\n",
            "Micro F1-score:  0.34996102883865937\n",
            "Macro F1-score:  0.3462394593823248\n"
          ]
        }
      ],
      "source": [
        "NNmodel=models(embedding_size_bag,x_train_bow,y_train_six_way,x_val_bow)\n",
        "y_test,y_pred=confusion_utility(x_test_bow,y_test_six_way,NNmodel)\n",
        "confusion = confusion_matrix(y_test, y_pred)\n",
        "print('Confusion Matrix\\n')\n",
        "print(confusion)\n",
        "print('\\nAccuracy: ',accuracy_score(y_test, y_pred))\n",
        "print('Micro F1-score: ',f1_score(y_test, y_pred, average='micro'))\n",
        "print('Macro F1-score: ',f1_score(y_test, y_pred, average='macro'))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Huux8sAHNXDE"
      },
      "source": [
        "**BERT Embedding on 6 way classifications**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LIljo3YNFYC9",
        "outputId": "dc26184d-b920-4850-d15b-01e374d3e77c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model: \"sequential_1\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " dense_5 (Dense)             (None, 1024)              399360    \n",
            "                                                                 \n",
            " dense_6 (Dense)             (None, 256)               262400    \n",
            "                                                                 \n",
            " dense_7 (Dense)             (None, 126)               32382     \n",
            "                                                                 \n",
            " dense_8 (Dense)             (None, 64)                8128      \n",
            "                                                                 \n",
            " dense_9 (Dense)             (None, 6)                 390       \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 702,660\n",
            "Trainable params: 702,660\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n",
            "Epoch 1/5\n",
            "321/321 [==============================] - 3s 6ms/step - loss: 2.4953 - accuracy: 0.1999 - val_loss: 1.7637 - val_accuracy: 0.1955\n",
            "Epoch 2/5\n",
            "321/321 [==============================] - 2s 5ms/step - loss: 1.7880 - accuracy: 0.1987 - val_loss: 1.7662 - val_accuracy: 0.1931\n",
            "Epoch 3/5\n",
            "321/321 [==============================] - 2s 5ms/step - loss: 1.7593 - accuracy: 0.2026 - val_loss: 1.7662 - val_accuracy: 0.1931\n",
            "Epoch 4/5\n",
            "321/321 [==============================] - 2s 5ms/step - loss: 1.7580 - accuracy: 0.2012 - val_loss: 1.7659 - val_accuracy: 0.1931\n",
            "Epoch 5/5\n",
            "321/321 [==============================] - 2s 5ms/step - loss: 1.7581 - accuracy: 0.2047 - val_loss: 1.7646 - val_accuracy: 0.1931\n",
            "Confusion Matrix\n",
            "\n",
            "[[  0   0   0 250   0   0]\n",
            " [  0   0   0 211   0   0]\n",
            " [  0   0   0 214   0   0]\n",
            " [  0   0   0 267   0   0]\n",
            " [  0   0   0 249   0   0]\n",
            " [  0   0   0  92   0   0]]\n",
            "\n",
            "Accuracy:  0.20810600155884645\n",
            "Micro F1-score:  0.20810600155884645\n",
            "Macro F1-score:  0.05741935483870967\n"
          ]
        }
      ],
      "source": [
        "NNmodel=models(embedding_size_bert,x_train_bert,y_train_six_way,x_val_bert)\n",
        "y_test,y_pred=confusion_utility(x_test_bert,y_test_six_way,NNmodel)\n",
        "\n",
        "confusion = confusion_matrix(y_test, y_pred)\n",
        "print('Confusion Matrix\\n')\n",
        "print(confusion)\n",
        "\n",
        "print('\\nAccuracy: ',accuracy_score(y_test, y_pred))\n",
        "print('Micro F1-score: ',f1_score(y_test, y_pred, average='micro'))\n",
        "print('Macro F1-score: ',f1_score(y_test, y_pred, average='macro'))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RGwYDeaON09u"
      },
      "source": [
        "**Glove encoding on 6 way Classification**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ctoTOw2uIK1G"
      },
      "source": [
        "# Binary Classification"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2ZJUrQ1SrEBa"
      },
      "source": [
        "## Preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "id": "eA3wQH1JinNx"
      },
      "outputs": [],
      "source": [
        "num_classes = 2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "id": "Mk-q1zwVF5KZ"
      },
      "outputs": [],
      "source": [
        "# Function for preprocessing labels\n",
        "def dataPreprocessingBinary(data):\n",
        "    y = data[\"label\"].tolist()\n",
        "\n",
        "    # Changing the 'half-true', 'mostly-true', barely-true', 'pants-fire' labels to True/False for Binary Classification\n",
        "    for x in range(len(y)):\n",
        "        if(y[x] == 'half-true'):\n",
        "            y[x] = 'True'\n",
        "        elif(y[x] == 'mostly-true'):\n",
        "            y[x] = 'True'\n",
        "        elif(y[x] == 'barely-true'):\n",
        "            y[x] = 'False'\n",
        "        elif(y[x] == 'pants-fire'):\n",
        "            y[x] = 'False'\n",
        "\n",
        "    # Converting the lables into binary class matrix\n",
        "    labelencoder_y = LabelEncoder()\n",
        "    y = labelencoder_y.fit_transform(y)\n",
        "    y = np_utils.to_categorical(y, num_classes)\n",
        "    return y"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "id": "REu1ue0xbuqp"
      },
      "outputs": [],
      "source": [
        "y_train_binary = dataPreprocessingBinary(train)\n",
        "y_test_binary = dataPreprocessingBinary(test)\n",
        "y_val_binary = dataPreprocessingBinary(val)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KI4PIrgR01Sd"
      },
      "source": [
        "## Model\n",
        "Build a model and pass bow, glove and bert embedded inputs: x_train_bow, x_train_glove, x_train_bert(similarly validate for val and report results on test)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OpKXBk6Z4PYd"
      },
      "source": [
        "**Bag of word embedding**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-J0inPaQb-8Y",
        "outputId": "46af2b51-d9f0-43d8-e9a3-eb694508db54"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model: \"sequential_2\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " dense_10 (Dense)            (None, 1024)              8198144   \n",
            "                                                                 \n",
            " dense_11 (Dense)            (None, 256)               262400    \n",
            "                                                                 \n",
            " dense_12 (Dense)            (None, 126)               32382     \n",
            "                                                                 \n",
            " dense_13 (Dense)            (None, 64)                8128      \n",
            "                                                                 \n",
            " dense_14 (Dense)            (None, 2)                 130       \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 8,501,184\n",
            "Trainable params: 8,501,184\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n",
            "Epoch 1/5\n",
            "321/321 [==============================] - 4s 11ms/step - loss: 2.1067 - accuracy: 0.5567 - val_loss: 0.6952 - val_accuracy: 0.5202\n",
            "Epoch 2/5\n",
            "321/321 [==============================] - 3s 10ms/step - loss: 0.6870 - accuracy: 0.5619 - val_loss: 0.6956 - val_accuracy: 0.5202\n",
            "Epoch 3/5\n",
            "321/321 [==============================] - 3s 10ms/step - loss: 0.6861 - accuracy: 0.5621 - val_loss: 0.6938 - val_accuracy: 0.5202\n",
            "Epoch 4/5\n",
            "321/321 [==============================] - 3s 10ms/step - loss: 0.6861 - accuracy: 0.5621 - val_loss: 0.6939 - val_accuracy: 0.5202\n",
            "Epoch 5/5\n",
            "321/321 [==============================] - 3s 10ms/step - loss: 0.6861 - accuracy: 0.5621 - val_loss: 0.6949 - val_accuracy: 0.5202\n",
            "Confusion Matrix\n",
            "\n",
            "[[  0 558]\n",
            " [  0 725]]\n",
            "\n",
            "Accuracy:  0.5650818394388153\n",
            "Micro F1-score:  0.5650818394388153\n",
            "Macro F1-score:  0.3610557768924303\n"
          ]
        }
      ],
      "source": [
        "## write your code here\n",
        "# Initialize hyperparameters\n",
        "# Create model\n",
        "# train\n",
        "# test\n",
        "# report accuracy, f1-score and confusion matrix\n",
        "\n",
        "NNmodel=models(embedding_size_bag,x_train_bow,y_train_binary,x_val_bow,False,y_val_binary)\n",
        "y_test,y_pred=confusion_utility(x_test_bow,y_train_binary,NNmodel)\n",
        "confusion = confusion_matrix(y_test, y_pred)\n",
        "print('Confusion Matrix\\n')\n",
        "print(confusion)\n",
        "print('\\nAccuracy: ',accuracy_score(y_test, y_pred))\n",
        "print('Micro F1-score: ',f1_score(y_test, y_pred, average='micro'))\n",
        "print('Macro F1-score: ',f1_score(y_test, y_pred, average='macro'))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OVhf8s2j4WK0"
      },
      "source": [
        "**Bert Embedding**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yy0ExnCvyQwf",
        "outputId": "c32b2888-16f3-49cd-c537-ef1928a832f2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model: \"sequential_3\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " dense_15 (Dense)            (None, 1024)              399360    \n",
            "                                                                 \n",
            " dense_16 (Dense)            (None, 256)               262400    \n",
            "                                                                 \n",
            " dense_17 (Dense)            (None, 126)               32382     \n",
            "                                                                 \n",
            " dense_18 (Dense)            (None, 64)                8128      \n",
            "                                                                 \n",
            " dense_19 (Dense)            (None, 2)                 130       \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 702,400\n",
            "Trainable params: 702,400\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n",
            "Epoch 1/5\n",
            "321/321 [==============================] - 2s 6ms/step - loss: 1.6960 - accuracy: 0.5582 - val_loss: 0.6947 - val_accuracy: 0.5202\n",
            "Epoch 2/5\n",
            "321/321 [==============================] - 2s 6ms/step - loss: 0.6864 - accuracy: 0.5623 - val_loss: 0.6946 - val_accuracy: 0.5202\n",
            "Epoch 3/5\n",
            "321/321 [==============================] - 2s 5ms/step - loss: 0.6858 - accuracy: 0.5624 - val_loss: 0.7028 - val_accuracy: 0.5202\n",
            "Epoch 4/5\n",
            "321/321 [==============================] - 2s 5ms/step - loss: 0.6855 - accuracy: 0.5625 - val_loss: 0.6935 - val_accuracy: 0.5202\n",
            "Epoch 5/5\n",
            "321/321 [==============================] - 2s 5ms/step - loss: 0.6854 - accuracy: 0.5625 - val_loss: 0.6943 - val_accuracy: 0.5202\n",
            "Confusion Matrix\n",
            "\n",
            "[[  0 558]\n",
            " [  0 725]]\n",
            "\n",
            "Accuracy:  0.5650818394388153\n",
            "Micro F1-score:  0.5650818394388153\n",
            "Macro F1-score:  0.3610557768924303\n"
          ]
        }
      ],
      "source": [
        "NNmodel=models(embedding_size_bert,x_train_bert,y_train_binary,x_val_bert,False,y_val_binary)\n",
        "y_test,y_pred=confusion_utility(x_test_bert,y_train_binary,NNmodel)\n",
        "confusion = confusion_matrix(y_test, y_pred)\n",
        "print('Confusion Matrix\\n')\n",
        "print(confusion)\n",
        "print('\\nAccuracy: ',accuracy_score(y_test, y_pred))\n",
        "print('Micro F1-score: ',f1_score(y_test, y_pred, average='micro'))\n",
        "print('Macro F1-score: ',f1_score(y_test, y_pred, average='macro'))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mYM51L-m96AB"
      },
      "source": [
        "**Difference between Encodings on classification**\n",
        "\n",
        "* Binary Classifier are performing better than 6 way classification problems. It is intuative as well, becuase at random binary have probability of 0.5 whereas 6 have have 0.17 probability.\n",
        "* Bert takes less space and performs equally well in case of binary classifier. I am not able to exploit the sequence in Bert by applying LSTM, in that case it can perform better.\n",
        "* For 6 way classifer, Bag of word is performing better than Bert. Since I have not used sequence of encodings well. In general Bert perform a better encoding than Bag of words."
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "name": "2021201034_Assignment3_Q3.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
