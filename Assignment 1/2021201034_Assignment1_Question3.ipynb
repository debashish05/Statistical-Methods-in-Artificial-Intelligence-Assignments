{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "02c54141",
      "metadata": {
        "id": "02c54141"
      },
      "source": [
        "## Spam Email Classifier with KNN using TF-IDF scores"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1c17102e",
      "metadata": {
        "id": "1c17102e"
      },
      "source": [
        "1.   Assignment must be implemented in Python 3 only.\n",
        "2.   You are allowed to use libraries for data preprocessing (numpy, pandas, nltk etc) and for evaluation metrics, data visualization (matplotlib etc.).\n",
        "3.   You will be evaluated not just on the overall performance of the model and also on the experimentation with hyper parameters, data prepossessing techniques etc.\n",
        "4.   The report file must be a well documented jupyter notebook, explaining the experiments you have performed, evaluation metrics and corresponding code. The code must run and be able to reproduce the accuracies, figures/graphs etc.\n",
        "5.   For all the questions, you must create a train-validation data split and test the hyperparameter tuning on the validation set. Your jupyter notebook must reflect the same.\n",
        "6.   Strict plagiarism checking will be done. An F will be awarded for plagiarism."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0d34a310",
      "metadata": {
        "id": "0d34a310"
      },
      "source": [
        "**Task: Given an email, classify it as spam or ham**\n",
        "\n",
        "Given input text file (\"emails.txt\") containing 5572 email messages, with each row having its corresponding label (spam/ham) attached to it.\n",
        "\n",
        "This task also requires basic pre-processing of text (like removing stopwords, stemming/lemmatizing, replacing email_address with 'email-tag', etc..).\n",
        "\n",
        "You are required to find the tf-idf scores for the given data and use them to perform KNN using Cosine Similarity."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b0c87696",
      "metadata": {
        "id": "b0c87696"
      },
      "source": [
        "### Import necessary libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 173,
      "id": "3d5a1fe2",
      "metadata": {
        "id": "3d5a1fe2"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import nltk\n",
        "import matplotlib.pyplot as plt\n",
        "import re\n",
        "import math\n",
        "import random\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sortedcontainers import SortedDict\n",
        "from sklearn import metrics\n",
        "from sklearn.neighbors import KNeighborsClassifier"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7aef4dff",
      "metadata": {
        "id": "7aef4dff"
      },
      "source": [
        "### Load dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 174,
      "id": "f178f892",
      "metadata": {
        "id": "f178f892",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 237
        },
        "outputId": "e9c87916-b469-44f1-da0c-66971b7a6d62"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-174-f6c4a6a1ceb6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m# data=pd.read_csv(url,delimiter = '\\n')\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mfile\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"emails.txt\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"r\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m       \u001b[0;31m# all dataset\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'emails.txt'"
          ]
        }
      ],
      "source": [
        "# url=\"https://github.com/debashish05/SMAI/blob/main/Dataset/emails.txt\"            # This will give the whole html page\n",
        "# url=\"https://raw.githubusercontent.com/debashish05/SMAI/main/Dataset/emails.txt\"  # serve unprocessed versions of files in GitHub.\n",
        "# data=pd.read_csv(url,delimiter = '\\n')\n",
        "\n",
        "file = open(\"emails.txt\",\"r\")\n",
        "\n",
        "data=[]       # all dataset\n",
        "\n",
        "for line in file:\n",
        "  data.append(line)\n",
        "\n",
        "file.close()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cd1ef5ba",
      "metadata": {
        "id": "cd1ef5ba"
      },
      "source": [
        "### Preprocess data\n",
        "1. Remove dupplicates,\n",
        "2. Converted all letters to lowercase letters, \n",
        "3. Converted email to emai tags,\n",
        "4. Remove puncuations, \n",
        "5. Removed stopwords and \n",
        "6. Lemmatized all words"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fd1733d7",
      "metadata": {
        "id": "fd1733d7"
      },
      "outputs": [],
      "source": [
        "# Remove Dupplicates\n",
        "def removeDupplicates(line):\n",
        "  \"\"\" Removes dupplicates element from the input provided\"\"\"\n",
        "  return list(set(line))\n",
        "\n",
        "\n",
        "# Removing stopwords\n",
        "def removeStopWord(lines, language=\"english\"):\n",
        "  \"\"\"\n",
        "      Removes stopword from the list of sentences, of the language passed\n",
        "  \"\"\"\n",
        "  nltk.download('stopwords')                          # Need to download stopwords first\n",
        "  stopWord = nltk.corpus.stopwords.words(language)    # all stopword in english\n",
        "\n",
        "  for i in range(len(lines)):\n",
        "    line=\"\"\n",
        "    for word in lines[i].split():\n",
        "      if word not in stopWord:\n",
        "        line+=word+\" \"\n",
        "    lines[i]=line\n",
        "  return lines\n",
        "\n",
        "# Lemmatization\n",
        "def lemmatization(lines):\n",
        "  \"\"\"\n",
        "      Lemmatizie all word in the lines, using WordNet\n",
        "  \"\"\"\n",
        "  \n",
        "  nltk.download('wordnet')\n",
        "  lemmatizer = nltk.stem.WordNetLemmatizer()\n",
        "  \n",
        "  for i in range(len(lines)):\n",
        "    line=\"\"\n",
        "    for word in lines[i].split():\n",
        "        line+=lemmatizer.lemmatize(word)+\" \"\n",
        "    lines[i]=line\n",
        "  \n",
        "  return lines\n",
        "\n",
        "# Convert all two lowercase for easier evaluation\n",
        "def lowerCase(lines):\n",
        "  \"\"\" Convert all the character in lines to lower case\"\"\"\n",
        "  return [line.lower() for line in lines]\n",
        "\n",
        "\n",
        "# identify all emai address and conver to tags\n",
        "def emailToTag(lines):\n",
        "  \"\"\"replacing email_address with 'email-tag'\"\"\"\n",
        "\n",
        "  emailRegex=re.compile(r'\\b[A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\\.[A-Z|a-z]{2,}\\b')\n",
        "  for i in range(len(lines)):\n",
        "    lines[i]=re.sub(emailRegex,r\"<email>\",lines[i])\n",
        "  return lines\n",
        "\n",
        "\n",
        "# remove puncuations\n",
        "def removePuncuations(lines):\n",
        "  \"\"\"Replacing punctuations with spaces\"\"\"\n",
        "\n",
        "  punc=re.compile(r'[?|!|\\'|\"|#|.|,|)|(|\\|/|:|)|;]')\n",
        "  for i in range(len(lines)):\n",
        "    lines[i]=re.sub(punc,r\" \",lines[i])\n",
        "  return lines\n",
        "\n",
        "\n",
        "data = removeDupplicates(data)                        \n",
        "data = lowerCase(data)\n",
        "data = emailToTag(data)\n",
        "data = removePuncuations(data)\n",
        "data = removeStopWord(data)\n",
        "data = lemmatization(data)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Term Frequencyâ€“Inverse Document Frequency**"
      ],
      "metadata": {
        "id": "qW-dDSZhv3gA"
      },
      "id": "qW-dDSZhv3gA"
    },
    {
      "cell_type": "code",
      "source": [
        "email=[]      # Contains email of all dataset\n",
        "output=[]     # represent ham or spam for corresponding email\n",
        "\n",
        "#random.shuffle(data)      # shuffling the data. \n",
        "\n",
        "for dataInstance in data:\n",
        "  if len(dataInstance) <=4:\n",
        "  # It may contain ham or spam only not the email after preprocessing\n",
        "    continue\n",
        "\n",
        "  result,mail = dataInstance.split(None,maxsplit = 1)  \n",
        "  # None implies Split on Whitespace include spaces, newlines \\n and tabs \\t , \n",
        "  # and consecutive whitespace are processed together.\n",
        "  email.append(mail)\n",
        "  output.append(result) \n",
        "\n",
        "\n",
        "\n",
        "idf={}        # If for all word present in corpus\n",
        "tf=[]         # List of dictionary, ith element have tf for ith docuements words\n",
        "N=len(data)\n",
        "\n",
        "\n",
        "# Term Frequency - Inverse Document frequency\n",
        "def TFIDF(lines):\n",
        "  \"\"\" \n",
        "      Computes Inverse Document frequency\n",
        "      IDF(wi,all corpus)= log(# document/ # dcouemnt which contain wi)\n",
        "      Calculates TF. Elements in list are treated as document and each word in \n",
        "      list is tread word of the documents.\n",
        "  \"\"\"\n",
        "  \n",
        "  for sentence in lines:\n",
        "    linefreq={}\n",
        "    linetf={}\n",
        "    count=0\n",
        "\n",
        "    for word in sentence.split():\n",
        "      linefreq.setdefault(word,0)\n",
        "      linefreq[word]+=1\n",
        "      count+=1\n",
        "\n",
        "    for key,value in linefreq.items():\n",
        "      idf.setdefault(key,0)\n",
        "      linetf.setdefault(key,0)\n",
        "      idf[key]+=1\n",
        "      linetf[key]+=value/count\n",
        "\n",
        "    tf.append(linetf)\n",
        "\n",
        "  for key,value in idf.items():\n",
        "    idf[key]=math.log(N/value,2)\n",
        "\n",
        "\n",
        "TFIDF(email)"
      ],
      "metadata": {
        "id": "4taEC235vvci"
      },
      "id": "4taEC235vvci",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Making Dictionary from the dataset provided**"
      ],
      "metadata": {
        "id": "v14KPTZbwCSt"
      },
      "id": "v14KPTZbwCSt"
    },
    {
      "cell_type": "code",
      "source": [
        "dictionary={} # word to corresponding index\n",
        "\n",
        "\n",
        "def makeDictionary(text):\n",
        "  \"\"\"\n",
        "    Make a hashmaof of strings and assign a unique number to it\n",
        "  \"\"\"\n",
        "  count=0\n",
        "  for sentence in text:\n",
        "    for word in sentence.split():\n",
        "      if word not in dictionary:\n",
        "        dictionary[word]=count\n",
        "        count+=1 \n",
        "\n",
        "makeDictionary(email)"
      ],
      "metadata": {
        "id": "gJMD2eqcwJ_f"
      },
      "id": "gJMD2eqcwJ_f",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Vectorization**"
      ],
      "metadata": {
        "id": "LhIbsjiawPZo"
      },
      "id": "LhIbsjiawPZo"
    },
    {
      "cell_type": "code",
      "source": [
        "# array of all email and corresponding tfidf value\n",
        "dataMatrix = []\n",
        "\n",
        "for i in range(len(email)):\n",
        "\n",
        "  vector=[0]*(len(dictionary)+1)\n",
        "  for word in email[i].split():\n",
        "    vector[dictionary[word]]=tf[i][word]*idf[word]\n",
        "  dataMatrix.append(np.array(vector))\n",
        "\n",
        "df ={'type':output,'tfidf':dataMatrix}\n",
        "df = pd.DataFrame(df)\n",
        "print(df)"
      ],
      "metadata": {
        "id": "e4mTYB57wYFH"
      },
      "id": "e4mTYB57wYFH",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "f76767a7",
      "metadata": {
        "id": "f76767a7"
      },
      "source": [
        "### Split data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f75e6cd2",
      "metadata": {
        "id": "f75e6cd2"
      },
      "outputs": [],
      "source": [
        "# 80% for Test Data, 10% Validation Data, 10% Data Set\n",
        "\n",
        "train, test = train_test_split(df, test_size=0.2)\n",
        "#test,validation = train_test_split(test, test_size=0.5)\n",
        "print(\"Number of train Data points\",len(train))\n",
        "print(\"Number of test Data points\",len(test))\n",
        "#print(\"Number of validation Data points\",len(validation))\n",
        "print(test)\n",
        "print(type(test))\n",
        "\n",
        "testArr = np.array(test['tfidf'])\n",
        "trainArr = np.array(train['tfidf'])\n",
        "#validationArr = np.array(validation['tfidf'])\n",
        "\n",
        "print(type(testArr))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ee6eb76b",
      "metadata": {
        "id": "ee6eb76b"
      },
      "source": [
        "### Train your KNN model (reuse previously iplemented model built from scratch) and test on your data"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "22baf6b2",
      "metadata": {
        "id": "22baf6b2"
      },
      "source": [
        "***1. Experiment with different distance measures [Euclidean distance, Manhattan distance, Hamming Distance] and compare with the Cosine Similarity distance results.***"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "68f1bb23",
      "metadata": {
        "id": "68f1bb23"
      },
      "outputs": [],
      "source": [
        "def euclideanDistance(a,b):\n",
        "  return np.sqrt(np.sum(np.square(b-a)))\n",
        "\n",
        "def cosineSimilarity(a,b):\n",
        "  return np.dot(b,a)/(np.linalg.norm(b)*np.linalg.norm(a))\n",
        "  \n",
        "def manhattanDistance(a,b):\n",
        "  return np.abs(a-b).sum()\n",
        "  \n",
        "\n",
        "# Preprocessing query for all value of k in validation dataset\n",
        "VcosineSimilarity = [] # ith element in the list, is a dictionary consiting of {entry number,cosine similarity}\n",
        "VmanhatanSimilarity = [] # ith element in the list, is a dictionary consiting of {entry number,manhatan similarity}\n",
        "VecludianSimilarity = [] #ith element in the list, is a dictionary consiting of {entry number,ecludian similarity}\n",
        "\n",
        "Csimilarity={}   \n",
        "Esimilarity={}\n",
        "Msimilarity={}\n",
        "\n",
        "for i in range(len(testArr)):\n",
        "  \n",
        "  Csimilarity.clear()   \n",
        "  Esimilarity.clear()\n",
        "  Msimilarity.clear()\n",
        "\n",
        "  for j in range(len(trainArr)):\n",
        "    Csimilarity[j] = (1-cosineSimilarity(testArr[i], trainArr[j]))\n",
        "    Msimilarity[j]= (manhattanDistance(testArr[i], trainArr[j]))\n",
        "    Esimilarity[j]= (euclideanDistance(testArr[i], trainArr[j]))\n",
        "  \n",
        "  CSim = dict(sorted(Csimilarity.items(), key=lambda x: x[1]))\n",
        "  ESim = dict(sorted(Esimilarity.items(), key=lambda x: x[1]))\n",
        "  MSim = dict(sorted(Msimilarity.items(), key=lambda x: x[1]))\n",
        "\n",
        "  VmanhatanSimilarity.append(MSim)\n",
        "  VcosineSimilarity.append(CSim)\n",
        "  VecludianSimilarity.append(ESim)\n",
        "  \n",
        "    \n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Consine Distance**"
      ],
      "metadata": {
        "id": "k3VG-FqMWb5T"
      },
      "id": "k3VG-FqMWb5T"
    },
    {
      "cell_type": "code",
      "source": [
        "Cscore=[]\n",
        "\n",
        "def KNNCosine(k):\n",
        "  predicted=[]\n",
        "  groundTruth=[]\n",
        "  for i in range(len(testArr)):\n",
        "    count=0\n",
        "    num=0\n",
        "    for j in VcosineSimilarity[i].keys():\n",
        "      if train.iloc[j]['type'] == 'spam':\n",
        "        count-=1\n",
        "      else:\n",
        "        count+=1\n",
        "      num+=1\n",
        "      if(num==k):\n",
        "        break\n",
        "    if count>0:\n",
        "      predicted.append(\"ham\")\n",
        "    else:\n",
        "      predicted.append(\"spam\")\n",
        "    \n",
        "    groundTruth.append(test.iloc[i]['type'])\n",
        "  \n",
        "  print(predicted)\n",
        "  print(groundTruth)\n",
        "  print(metrics.confusion_matrix(groundTruth,predicted, labels=[\"ham\",\"spam\"]))\n",
        "  print(metrics.classification_report(groundTruth,predicted,labels=[\"ham\",\"spam\"],zero_division=1))\n",
        "  Cscore.append(metrics.f1_score(groundTruth,predicted,average=\"micro\"))\n"
      ],
      "metadata": {
        "id": "1rB8gu2gL5l8"
      },
      "id": "1rB8gu2gL5l8",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Manhatan Distance**"
      ],
      "metadata": {
        "id": "v5bQfTJmQFgI"
      },
      "id": "v5bQfTJmQFgI"
    },
    {
      "cell_type": "code",
      "source": [
        "Mscore=[]\n",
        "\n",
        "def KNNManhatan(k):\n",
        "  predicted=[]\n",
        "  groundTruth=[]\n",
        "  for i in range(len(testArr)):\n",
        "    count=0\n",
        "    num=0\n",
        "    for j in VmanhatanSimilarity[i].keys():\n",
        "      if train.iloc[j]['type'] == 'spam':\n",
        "        count-=1\n",
        "      else:\n",
        "        count+=1\n",
        "      num+=1\n",
        "      if(num==k):\n",
        "        break\n",
        "    if count>0:\n",
        "      predicted.append(\"ham\")\n",
        "    else:\n",
        "      predicted.append(\"spam\")\n",
        "    \n",
        "    groundTruth.append(test.iloc[i]['type'])\n",
        "  \n",
        "  print(predicted)\n",
        "  print(groundTruth)\n",
        "  print(metrics.confusion_matrix(groundTruth,predicted, labels=[\"ham\",\"spam\"]))\n",
        "  print(metrics.classification_report(groundTruth,predicted,labels=[\"ham\",\"spam\"],zero_division=1))\n",
        "  Mscore.append(metrics.f1_score(groundTruth,predicted,average=\"micro\"))"
      ],
      "metadata": {
        "id": "phrvuTRjQC8s"
      },
      "id": "phrvuTRjQC8s",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Euclidian Distance**"
      ],
      "metadata": {
        "id": "XkE2wQ8RQtVV"
      },
      "id": "XkE2wQ8RQtVV"
    },
    {
      "cell_type": "code",
      "source": [
        "Escore=[]\n",
        "\n",
        "def KNNEuclid(k):\n",
        "  predicted=[]\n",
        "  groundTruth=[]\n",
        "  for i in range(len(testArr)):\n",
        "    count=0\n",
        "    num=0\n",
        "    for j in VecludianSimilarity[i].keys():\n",
        "      if train.iloc[j]['type'] == 'spam':\n",
        "        count-=1\n",
        "      else:\n",
        "        count+=1\n",
        "      num+=1\n",
        "      if(num==k):\n",
        "        break\n",
        "    if count>0:\n",
        "      predicted.append(\"ham\")\n",
        "    else:\n",
        "      predicted.append(\"spam\")\n",
        "    \n",
        "    groundTruth.append(test.iloc[i]['type'])\n",
        "  \n",
        "  print(metrics.confusion_matrix(groundTruth,predicted, labels=[\"ham\",\"spam\"]))\n",
        "  print(metrics.classification_report(groundTruth,predicted,labels=[\"ham\",\"spam\"],zero_division=1))\n",
        "  Escore.append(metrics.f1_score(groundTruth,predicted,average=\"micro\"))"
      ],
      "metadata": {
        "id": "r0ql3Dg0QxUW"
      },
      "id": "r0ql3Dg0QxUW",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "bcf6f3b1",
      "metadata": {
        "id": "bcf6f3b1"
      },
      "source": [
        "***2. Explain which distance measure works best and why? Explore the distance measures and weigh their pro and cons in different application settings.***"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "bae57a01",
      "metadata": {
        "id": "bae57a01"
      },
      "source": [
        "Cosine Gives Best result in my calculations. Consie is 0 when both the vector points are perpendicular to each other and 1 when they lie on each other in the hyperplane. Cosine distance best work in collabrative filtering and recommendations systems. Whereas Manhatna distance work best when we are working in a grid. Hamming distance in best suited when we are dealing with bits of number (where the input is binary, like spam or ham etc). Euclidian distance find the shortest distance between any two points in the hyper plane. When the dimensionality increase manhattan and euclidian distance can perform better. Since two points may be on the same line but far part in distance. "
      ]
    },
    {
      "cell_type": "markdown",
      "id": "45a99c76",
      "metadata": {
        "id": "45a99c76"
      },
      "source": [
        "***3. Report Cosine, Euclidian, Manhatan score in a tabular form***\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e9668814",
      "metadata": {
        "id": "e9668814"
      },
      "outputs": [],
      "source": [
        "knn_val=[1,3,5,7,9,11,17,23,28]\n",
        "for val in knn_val:\n",
        "  KNNCosine(val)\n",
        "  KNNManhatan(val)\n",
        "  KNNEuclid(val)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d4dde8d3",
      "metadata": {
        "id": "d4dde8d3"
      },
      "source": [
        "***4. Choose different K values (k=1,3,5,7,11,17,23,28) and experiment. Plot a graph showing F1 score vs k.***"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "45e0fd2f",
      "metadata": {
        "id": "45e0fd2f"
      },
      "outputs": [],
      "source": [
        "plt.plot(knn_val,Cscore)\n",
        "plt.xlabel(\"K values\")\n",
        "plt.ylabel(\"F1 score\")\n",
        "plt.title(\"Cosine Similarity\")\n",
        "plt.show()\n",
        "\n",
        "plt.plot(knn_val,Mscore)\n",
        "plt.xlabel(\"K values\")\n",
        "plt.ylabel(\"F1 score\")\n",
        "plt.title(\"Mahantan Distance Similarity\")\n",
        "plt.show()\n",
        "\n",
        "plt.plot(knn_val,Escore)\n",
        "plt.xlabel(\"K values\")\n",
        "plt.ylabel(\"F1 score\")\n",
        "plt.title(\"Euclidian Distance Similarity\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "15000352",
      "metadata": {
        "id": "15000352"
      },
      "source": [
        "### Train and test Sklearn's KNN classifier model on your data (use metric which gave best results on your experimentation with built-from-scratch model.)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "48aab7d3",
      "metadata": {
        "id": "48aab7d3"
      },
      "outputs": [],
      "source": [
        "score=[]\n",
        "\n",
        "for val in knn_val:\n",
        "    knnModel = KNeighborsClassifier(val, metric='cosine')\n",
        "    final = knnModel.fit(list(train['tfidf']), list(train['type']))\n",
        "    predicted = final.predict(list(test['tfidf']))\n",
        "    #score.append(metrics.f1_score(list(test['type']), list(predicted)))\n",
        "    #score.append(metrics.f1_score(list(test['type']), list(predicted), average=None))\n",
        "    score.append(metrics.f1_score(list(test['type']), list(predicted), average=\"micro\"))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7d22aa47",
      "metadata": {
        "id": "7d22aa47"
      },
      "source": [
        "***Compare both the models result.***"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can see that SKLearn has lesser score at some point of time value 28 rest they are same.  "
      ],
      "metadata": {
        "id": "SVXyl4ROy3Pt"
      },
      "id": "SVXyl4ROy3Pt"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6a7a5274",
      "metadata": {
        "id": "6a7a5274"
      },
      "outputs": [],
      "source": [
        "plt.plot(knn_val, score, Label=\"Sklearn's KNN\")\n",
        "plt.plot(knn_val,Cscore,Label=\"Cosine Similarity\")\n",
        "plt.xlabel('k - value')\n",
        "plt.ylabel('F1 Score')\n",
        "plt.title('Comparison with Sklearn KNN classifier and  built-from-scratch model ')\n",
        "plt.legend()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "64987575",
      "metadata": {
        "id": "64987575"
      },
      "source": [
        "***What is the time complexity of training using KNN classifier?***"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2770c106",
      "metadata": {
        "id": "2770c106"
      },
      "source": [
        "KNN take O(1) for training because we just need to add one point in the plane. "
      ]
    },
    {
      "cell_type": "markdown",
      "id": "fad1f345",
      "metadata": {
        "id": "fad1f345"
      },
      "source": [
        "***What is the time complexity while testing? Is KNN a linear classifier or can it learn any boundary?***"
      ]
    },
    {
      "cell_type": "raw",
      "id": "0daaa324",
      "metadata": {
        "id": "0daaa324"
      },
      "source": [
        "While finding KNN for a point it take O(d*n+klogn), where n is the number of data points and k is the value and d is the size of the dimension. KNN is not a linear classifier and it has a tendency to learn any boundary."
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.10"
    },
    "colab": {
      "name": "2021201034_Assignment1_Question3.ipynb",
      "provenance": [],
      "collapsed_sections": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}