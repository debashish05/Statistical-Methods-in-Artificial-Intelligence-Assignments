{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "improving-pepper",
   "metadata": {},
   "source": [
    "# Assignment 2 - Question 2\n",
    "The objective of this assignment is to get you familiarize with  the  problem  of  `Linear Regression`.\n",
    "\n",
    "## Instructions\n",
    "- Write your code and analysis in the indicated cells.\n",
    "- Ensure that this notebook runs without errors when the cells are run in sequence.\n",
    "- Do not attempt to change the contents of other cells.\n",
    "- No inbuilt functions to be used until specified\n",
    "\n",
    "## Submission\n",
    "- Ensure that this notebook runs without errors when the cells are run in sequence.\n",
    "- Rename the notebook to `<roll_number>_Assignment2_Q2.ipynb`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "final-transaction",
   "metadata": {},
   "source": [
    "## 2.0 Background about the dataset\n",
    "\n",
    "TLDR: You have 4 independent variables (`float`) for each molecule. You can use a linear combination of these 4 independent variables to predict the bandgap (dependent variable) of each molecule.\n",
    "\n",
    "You can read more about the problem in [Li et al, Bandgap tuning strategy by cations and halide ions of lead halide perovskites learned from machine learning, RSC Adv., 2021,11, 15688-15694](https://doi.org/10.1039/D1RA03117A)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "lyric-olympus",
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import random\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "hundred-receipt",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_molecules = list()\n",
    "\n",
    "with open('bg_data.txt', 'r') as infile:\n",
    "    input_rows = csv.DictReader(infile)\n",
    "    \n",
    "    for row in input_rows:\n",
    "        current_mol = ([float(row['Cs']), float(row['FA']), float(row['Cl']), float(row['Br'])], float(row['Bandgap']))\n",
    "        all_molecules.append(current_mol)\n",
    "\n",
    "random.shuffle(all_molecules)\n",
    "\n",
    "\n",
    "num_train = int(len(all_molecules) * 0.8)\n",
    "\n",
    "# each point in x_train has 4 values - 1 for each feature\n",
    "x_train = [x[0] for x in all_molecules[:num_train]]\n",
    "# each point in y_train has 1 value - the bandgap of the molecule\n",
    "y_train = [x[1] for x in all_molecules[:num_train]]\n",
    "\n",
    "x_test = [x[0] for x in all_molecules[num_train:]]\n",
    "y_test = [x[1] for x in all_molecules[num_train:]]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "square-direction",
   "metadata": {},
   "source": [
    "### 2.1 Implement a Linear Regression model that minimizes the MSE **without using any libraries**. You may use NumPy to vectorize your code, but *do not use numpy.polyfit* or anything similar.\n",
    "\n",
    "2.1.1 Explain how you plan to implement Linear Regression in 5-10 lines.\n",
    "\n",
    "To implement linear regression, I have used following steps:\n",
    "* Defined a function, which can predict the output given input parameter and model(coefficient here). \n",
    "* For the cost, function I have normally used squared difference between hypothesis and ground truth results.\n",
    "* I have started with null vector at starting point. \n",
    "* I have done 1000 iteration over the training set. \n",
    "* I have used stochastic gradient descent. So after every point, I am updating the paramaeters. \n",
    "* After going through a training point. I update the coefficients. \n",
    "* I have taken a extra parameter, to be treated as bias\n",
    "* After all the ephocs, I will return model (coefficient of the eqaution) and the total error. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "frozen-forth",
   "metadata": {},
   "source": [
    "<!-- your answer to 1.1.1 -->"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "addressed-winter",
   "metadata": {},
   "source": [
    "2.1.2 Implement Linear Regression using `x_train` and `y_train` as the train dataset.\n",
    "\n",
    "2.1.2.1 Choose the best learning rate and print the learning rate for which you achieved the best MSE.\n",
    "\n",
    "* The best values of alpha comes out to be 0.01. Since number of training points are less, we can take lesser values of alpha and still the time will be less. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "angry-depression",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of training points:  87\n",
      "Error in 1 iteration:  0.49722505553654756\n",
      "Error in 2 iteration:  0.5057743930070934\n",
      "Error in 3 iteration:  0.513204918481948\n",
      "Error in 4 iteration:  0.519913258165878\n",
      "Error in 5 iteration:  0.5262164176896804\n",
      "Error in 6 iteration:  0.5323227594419991\n",
      "Error in 7 iteration:  0.5383638607079492\n",
      "Error in 8 iteration:  0.5444265809363583\n",
      "Error in 9 iteration:  0.5505715144750465\n",
      "Error in 10 iteration:  0.5568419804190452\n",
      "\n",
      "Best Learning Rate is:  0.01  With Total Error:  0.49722505553654756\n",
      "Best Coefficient:  [1.5792035421874069, -0.02769275845302008, -0.1147017137153846, 1.5347605752116404, 0.6519196408356728]\n"
     ]
    }
   ],
   "source": [
    "# implement Linear Regression\n",
    "from random import randrange\n",
    "print(\"Number of training points: \",len(y_train))\n",
    "\n",
    "def prediction(coefficient,inputVector):\n",
    "    \"\"\"Will predict the output given coefficent and vector\"\"\"\n",
    "    predict=0\n",
    "    for i in range(len(coefficient)):\n",
    "        if i==0:\n",
    "            predict+=coefficient[0]\n",
    "        else:\n",
    "            predict+=coefficient[i]*inputVector[i-1]\n",
    "    \n",
    "    return predict\n",
    "\n",
    "\n",
    "\n",
    "# Linear regression using stochastic gradient descent\n",
    "\n",
    "def linearReg(alpha,x_train,y_train,epoch):\n",
    "    \"\"\" alpha is the learnign rate. Epoch is the number of times entire training data is parsed\"\"\"\n",
    "    coefficient=[0 for i in range(len(x_train[0])+1)]\n",
    "    # start with a random value\n",
    "\n",
    "    for epochs in range(epoch):\n",
    "        Totalerror=0\n",
    "        for i in range(len(x_train)):       #for each data point\n",
    "            predict=prediction(coefficient,x_train[i])\n",
    "            error=(predict-y_train[i])      \n",
    "            Totalerror+=error**2            # Square error\n",
    "            \n",
    "            # update the coefficient\n",
    "            for j in range(len(x_train[i])+1):\n",
    "                if j==0:\n",
    "                    coefficient[0]-=alpha*error\n",
    "                else:\n",
    "                    coefficient[j]-=alpha*x_train[i][j-1]*error\n",
    "\n",
    "        #print(\"Epoch: \",epochs, \" Sum Error: \",Totalerror,\" Learning Rate: \", alpha)\n",
    "\n",
    "    return coefficient,Totalerror\n",
    "\n",
    "\n",
    "alpha=0.01\n",
    "epoch=1000\n",
    "bestLearningRate=alpha\n",
    "leastError=float('inf')\n",
    "bestCoordinate=[]\n",
    "while alpha<0.1:\n",
    "    coefficient,Totalerror=linearReg(alpha,x_train,y_train,epoch)\n",
    "    print(\"Error in\",int(alpha*100),\"iteration: \",Totalerror)\n",
    "    if Totalerror<leastError:\n",
    "        leastError=Totalerror\n",
    "        bestLearningRate=alpha\n",
    "        bestCoordinate=coefficient\n",
    "    alpha+=0.01\n",
    "\n",
    "    \n",
    "print(\"\\nBest Learning Rate is: \",bestLearningRate,\" With Total Error: \",leastError)\n",
    "print(\"Best Coefficient: \",coefficient)\n",
    "\n",
    "\n",
    "y_pred=[]\n",
    "\n",
    "for entry in x_test:\n",
    "    y_pred.append(prediction(bestCoordinate,entry))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "angry-tampa",
   "metadata": {},
   "source": [
    "2.1.3 Make a [Parity Plot](https://en.wikipedia.org/wiki/Parity_plot) of your model's bandgap predictions on the test set with the actual values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "foster-center",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x1440 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Get the predictions of x_test into `y_pred`\n",
    "\n",
    "#\n",
    "# ...\n",
    "#\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "fig, ax = plt.subplots(figsize=(10,20))\n",
    "\n",
    "ax.scatter(y_test, y_pred)\n",
    "\n",
    "lims = [\n",
    "    np.min([ax.get_xlim(), ax.get_ylim()]),\n",
    "    np.max([ax.get_xlim(), ax.get_ylim()]),\n",
    "]\n",
    "ax.plot(lims, lims, 'k-', alpha=0.75, zorder=0)\n",
    "ax.set_aspect('equal')\n",
    "ax.set_xlim(lims)\n",
    "ax.set_ylim(lims)\n",
    "\n",
    "ax.set_title('Parity Plot of Custom Linear Regression')\n",
    "ax.set_xlabel('Ground truth bandgap values')\n",
    "ax.set_ylabel('Predicted bandgap values')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dominant-chaos",
   "metadata": {},
   "source": [
    "### 2.2 Implement Ridge regression\n",
    "2.2.1 Explain Ridge regression briefly in 1-2 lines.\n",
    "\n",
    "* Ridge Regression aka L2 regularization. It is used to avoid overfitting of the data. It uses a parameter lambda for regularization. So it is linear regression cost fucntion + lambda*[sum over square of all coeff.]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "happy-cyprus",
   "metadata": {},
   "source": [
    "<!-- -->"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "tight-forwarding",
   "metadata": {},
   "source": [
    "2.2.2 Implement Ridge regression and make a table of different RMSE scores you achieved with different values of alpha. What does the parameter `alpha` do? How does it affect the results here? Explain in 5-10 lines in total. (You can use scikit-learn from this cell onwards)\n",
    "* Alpha is the Regularization strength or penalty. It encourgaes to use smaller parameters. It is used to reduces the variance of the predictions and avoid overfitting. Larger values of alpha signifies stronger regularization. For very high values it might underfit as well.\n",
    "* Alpha value at 0.001 gives the minimum error on the test data. As we keep on increasing it, the test error is increasing by small margin. Due to smaller dataset size, we won't able to see much impact. \n",
    "* Ridge regression can never make any parameter zero, it can minimise some but not make it exactly zero. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "violent-northern",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Learning Rate\tAverage Squared Error\n",
      "0.001 \t\t 3.8401919715167607\n",
      "0.006 \t\t 3.840217558562703\n",
      "0.011 \t\t 3.840243142954789\n",
      "0.016 \t\t 3.8402687246934275\n",
      "0.021 \t\t 3.840294303779029\n",
      "0.026 \t\t 3.8403198802120033\n",
      "0.031 \t\t 3.840345453992758\n",
      "0.036 \t\t 3.840371025121705\n",
      "0.041 \t\t 3.8403965935992512\n",
      "0.046 \t\t 3.840422159425805\n",
      "0.051 \t\t 3.8404477226017772\n",
      "0.056 \t\t 3.840473283127575\n",
      "0.061 \t\t 3.8404988410036083\n",
      "0.066 \t\t 3.8405243962302844\n",
      "0.071 \t\t 3.8405499488080137\n",
      "0.076 \t\t 3.8405754987372025\n",
      "0.081 \t\t 3.840601046018261\n",
      "0.086 \t\t 3.840626590651596\n",
      "0.091 \t\t 3.8406521326376164\n",
      "0.096 \t\t 3.840677671976732\n",
      "0.101 \t\t 3.840703208669349\n",
      "0.106 \t\t 3.8407287427158736\n",
      "0.111 \t\t 3.8407542741167173\n",
      "0.116 \t\t 3.8407798028722864\n",
      "0.121 \t\t 3.840805328982988\n",
      "0.126 \t\t 3.8408308524492294\n",
      "0.131 \t\t 3.8408563732714214\n",
      "0.136 \t\t 3.8408818914499663\n",
      "0.141 \t\t 3.8409074069852753\n",
      "0.146 \t\t 3.8409329198777553\n",
      "0.151 \t\t 3.840958430127813\n",
      "0.156 \t\t 3.840983937735855\n",
      "0.161 \t\t 3.8410094427022887\n",
      "0.166 \t\t 3.8410349450275216\n",
      "0.171 \t\t 3.8410604447119585\n",
      "0.176 \t\t 3.841085941756008\n",
      "0.181 \t\t 3.8411114361600798\n",
      "0.186 \t\t 3.8411369279245733\n",
      "0.191 \t\t 3.841162417049903\n",
      "0.196 \t\t 3.84118790353647\n",
      "0.201 \t\t 3.8412133873846823\n",
      "0.206 \t\t 3.8412388685949463\n",
      "0.211 \t\t 3.8412643471676673\n",
      "0.216 \t\t 3.8412898231032524\n",
      "0.221 \t\t 3.8413152964021084\n",
      "0.226 \t\t 3.8413407670646404\n",
      "0.231 \t\t 3.8413662350912534\n",
      "0.236 \t\t 3.841391700482355\n",
      "0.241 \t\t 3.841417163238349\n",
      "0.246 \t\t 3.841442623359643\n",
      "0.251 \t\t 3.84146808084664\n",
      "0.256 \t\t 3.84149353569975\n",
      "0.261 \t\t 3.841518987919375\n",
      "0.266 \t\t 3.8415444375059193\n",
      "0.271 \t\t 3.8415698844597914\n",
      "0.276 \t\t 3.841595328781393\n",
      "0.281 \t\t 3.841620770471134\n",
      "0.286 \t\t 3.8416462095294133\n",
      "0.291 \t\t 3.8416716459566405\n",
      "0.296 \t\t 3.841697079753219\n",
      "0.301 \t\t 3.8417225109195527\n",
      "0.306 \t\t 3.8417479394560483\n",
      "0.311 \t\t 3.8417733653631077\n",
      "0.316 \t\t 3.841798788641138\n",
      "0.321 \t\t 3.8418242092905426\n",
      "0.326 \t\t 3.841849627311723\n",
      "0.331 \t\t 3.841875042705089\n",
      "0.336 \t\t 3.8419004554710408\n",
      "0.341 \t\t 3.841925865609983\n",
      "0.346 \t\t 3.8419512731223207\n",
      "0.351 \t\t 3.8419766780084577\n",
      "0.356 \t\t 3.8420020802687973\n",
      "0.361 \t\t 3.8420274799037446\n",
      "0.366 \t\t 3.842052876913702\n",
      "0.371 \t\t 3.8420782712990724\n",
      "0.376 \t\t 3.842103663060259\n",
      "0.381 \t\t 3.8421290521976696\n",
      "0.386 \t\t 3.842154438711701\n",
      "0.391 \t\t 3.8421798226027617\n",
      "0.396 \t\t 3.842205203871255\n",
      "0.401 \t\t 3.842230582517578\n",
      "0.406 \t\t 3.842255958542141\n",
      "0.411 \t\t 3.8422813319453435\n",
      "0.416 \t\t 3.8423067027275866\n",
      "0.421 \t\t 3.8423320708892774\n",
      "0.426 \t\t 3.8423574364308153\n",
      "0.431 \t\t 3.8423827993526056\n",
      "0.436 \t\t 3.842408159655047\n",
      "0.441 \t\t 3.8424335173385447\n",
      "0.446 \t\t 3.842458872403502\n",
      "0.451 \t\t 3.8424842248503186\n",
      "0.456 \t\t 3.8425095746793985\n",
      "0.461 \t\t 3.842534921891145\n",
      "0.466 \t\t 3.8425602664859566\n",
      "0.471 \t\t 3.842585608464236\n",
      "0.476 \t\t 3.8426109478263886\n",
      "0.481 \t\t 3.8426362845728126\n",
      "0.486 \t\t 3.8426616187039104\n",
      "0.491 \t\t 3.8426869502200858\n",
      "0.496 \t\t 3.8427122791217374\n",
      "0.501 \t\t 3.8427376054092695\n",
      "0.506 \t\t 3.842762929083082\n",
      "0.511 \t\t 3.8427882501435757\n",
      "0.516 \t\t 3.842813568591151\n",
      "0.521 \t\t 3.842838884426214\n",
      "0.526 \t\t 3.8428641976491593\n",
      "0.531 \t\t 3.8428895082603933\n",
      "0.536 \t\t 3.842914816260313\n",
      "0.541 \t\t 3.8429401216493217\n",
      "0.546 \t\t 3.8429654244278195\n",
      "0.551 \t\t 3.8429907245962056\n",
      "0.556 \t\t 3.843016022154883\n",
      "0.561 \t\t 3.8430413171042512\n",
      "0.566 \t\t 3.8430666094447092\n",
      "0.571 \t\t 3.843091899176659\n",
      "0.576 \t\t 3.8431171863005007\n",
      "0.581 \t\t 3.8431424708166375\n",
      "0.586 \t\t 3.8431677527254617\n",
      "0.591 \t\t 3.843193032027381\n",
      "0.596 \t\t 3.8432183087227902\n",
      "0.601 \t\t 3.8432435828120934\n",
      "0.606 \t\t 3.8432688542956868\n",
      "0.611 \t\t 3.8432941231739735\n",
      "0.616 \t\t 3.843319389447348\n",
      "0.621 \t\t 3.8433446531162163\n",
      "0.626 \t\t 3.843369914180974\n",
      "0.631 \t\t 3.8433951726420204\n",
      "0.636 \t\t 3.8434204284997566\n",
      "0.641 \t\t 3.8434456817545803\n",
      "0.646 \t\t 3.8434709324068916\n",
      "0.651 \t\t 3.843496180457088\n",
      "0.656 \t\t 3.84352142590557\n",
      "0.661 \t\t 3.843546668752736\n",
      "0.666 \t\t 3.8435719089989857\n",
      "0.671 \t\t 3.8435971466447167\n",
      "0.676 \t\t 3.8436223816903294\n",
      "0.681 \t\t 3.8436476141362195\n",
      "0.686 \t\t 3.8436728439827874\n",
      "0.691 \t\t 3.8436980712304307\n",
      "0.696 \t\t 3.8437232958795495\n",
      "0.701 \t\t 3.8437485179305386\n",
      "0.706 \t\t 3.8437737373838003\n",
      "0.711 \t\t 3.843798954239731\n",
      "0.716 \t\t 3.8438241684987275\n",
      "0.721 \t\t 3.8438493801611884\n",
      "0.726 \t\t 3.8438745892275126\n",
      "0.731 \t\t 3.843899795698098\n",
      "0.736 \t\t 3.843924999573341\n",
      "0.741 \t\t 3.843950200853638\n",
      "0.746 \t\t 3.8439753995393904\n",
      "0.751 \t\t 3.8440005956309915\n",
      "0.756 \t\t 3.844025789128842\n",
      "0.761 \t\t 3.844050980033337\n",
      "0.766 \t\t 3.8440761683448765\n",
      "0.771 \t\t 3.8441013540638536\n",
      "0.776 \t\t 3.844126537190668\n",
      "0.781 \t\t 3.844151717725716\n",
      "0.786 \t\t 3.844176895669396\n",
      "0.791 \t\t 3.844202071022103\n",
      "0.796 \t\t 3.8442272437842333\n",
      "0.801 \t\t 3.8442524139561853\n",
      "0.806 \t\t 3.8442775815383543\n",
      "0.811 \t\t 3.8443027465311377\n",
      "0.816 \t\t 3.8443279089349316\n",
      "0.821 \t\t 3.8443530687501313\n",
      "0.826 \t\t 3.844378225977134\n",
      "0.831 \t\t 3.8444033806163374\n",
      "0.836 \t\t 3.8444285326681356\n",
      "0.841 \t\t 3.844453682132925\n",
      "0.846 \t\t 3.8444788290111003\n",
      "0.851 \t\t 3.84450397330306\n",
      "0.856 \t\t 3.8445291150091974\n",
      "0.861 \t\t 3.844554254129911\n",
      "0.866 \t\t 3.8445793906655936\n",
      "0.871 \t\t 3.844604524616642\n",
      "0.876 \t\t 3.844629655983453\n",
      "0.881 \t\t 3.844654784766418\n",
      "0.886 \t\t 3.8446799109659366\n",
      "0.891 \t\t 3.8447050345824034\n",
      "0.896 \t\t 3.8447301556162103\n",
      "0.901 \t\t 3.8447552740677553\n",
      "0.906 \t\t 3.8447803899374335\n",
      "0.911 \t\t 3.8448055032256363\n",
      "0.916 \t\t 3.844830613932764\n",
      "0.921 \t\t 3.8448557220592066\n",
      "0.926 \t\t 3.8448808276053623\n",
      "0.931 \t\t 3.844905930571623\n",
      "0.936 \t\t 3.8449310309583837\n",
      "0.941 \t\t 3.84495612876604\n",
      "0.946 \t\t 3.844981223994985\n",
      "0.951 \t\t 3.8450063166456157\n",
      "0.956 \t\t 3.8450314067183213\n",
      "0.961 \t\t 3.8450564942135013\n",
      "0.966 \t\t 3.8450815791315467\n",
      "0.971 \t\t 3.8451066614728506\n",
      "0.976 \t\t 3.845131741237809\n",
      "0.981 \t\t 3.8451568184268154\n",
      "0.986 \t\t 3.845181893040263\n",
      "0.991 \t\t 3.8452069650785448\n",
      "0.996 \t\t 3.8452320345420548\n"
     ]
    }
   ],
   "source": [
    "# you should not have imported sklearn before this point\n",
    "# implement Ridge regression and make a table where you explore the effect of different values of `alpha`\n",
    "\n",
    "from sklearn.linear_model import Ridge\n",
    "\n",
    "alpha=[]\n",
    "curr=0.001\n",
    "while curr<1:\n",
    "    alpha.append(curr)\n",
    "    curr+=0.005\n",
    "\n",
    "n_samples=len(x_train)\n",
    "n_features = len(x_train[0])\n",
    "errorForAlpha=[]\n",
    "\n",
    "for lrate in alpha:\n",
    "    rng = np.random.RandomState(0)\n",
    "    clf = Ridge(alpha=lrate)\n",
    "    clf.fit(rng.randn(n_samples, n_features),rng.randn(n_samples))\n",
    "    output=clf.predict(x_test)\n",
    "    \n",
    "    sqaurError=0\n",
    "    for i in range(len(output)):\n",
    "        sqaurError+=(output[i]-y_test[i])**2\n",
    "    sqaurError/=len(output)\n",
    "    \n",
    "    errorForAlpha.append(sqaurError)\n",
    "\n",
    "print(\"Learning Rate\\tAverage Squared Error\")\n",
    "for i in range(len(alpha)):\n",
    "    print(round(alpha[i],3),\"\\t\\t\",errorForAlpha[i])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adolescent-temperature",
   "metadata": {},
   "source": [
    "### 2.3 Implement Lasso regression\n",
    "2.3.1 Explain Lasso regression briefly in 1-2 lines.\n",
    "\n",
    "* Full form of LASSO is Least Absolute Shrinkage and Selection operator. Aka. L1 regularization.\n",
    "* Here the bias added is the lambda times the sum of absolute value of coefficients. It tries to solve the problem of overfitting. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "popular-wonder",
   "metadata": {},
   "source": [
    "2.3.2 Implement Lasso regression and make a table of different RMSE scores you achieved with different values of alpha. What does the parameter `alpha` do? How does it affect the results here? Explain in 5-10 lines in total.\n",
    "* Parameter alpha denotes the weightage given to extra bias. It calculate the regression as linear regression + alpha *(sum of absolute value of all coefficient). \n",
    "* For alpha = 0.001 it gives the least training error. And keep on increasing for higher values by a very small margin. \n",
    "* Lasso can make the coeffient of the expression and totally remnoving some attribute for large values of alpha. L2 Can't able to achieve this. \n",
    "* Lasso tries to decrease the variance by adding a small bias. \n",
    "* In the tabel all values are more or less the same for error part. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "extra-brighton",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Learning Rate\tAverage Squared Error\n",
      "0.001 \t\t 0.005157560457848089\n",
      "0.006 \t\t 0.004965682475124503\n",
      "0.011 \t\t 0.006352913948151295\n",
      "0.016 \t\t 0.00933495857010835\n",
      "0.021 \t\t 0.013911817065936688\n",
      "0.026 \t\t 0.020083489435636353\n",
      "0.031 \t\t 0.02784997567920733\n",
      "0.036 \t\t 0.037211275796649596\n",
      "0.041 \t\t 0.04816738978796316\n",
      "0.046 \t\t 0.060718320649142704\n",
      "0.051 \t\t 0.0746093624302212\n",
      "0.056 \t\t 0.08471567893669432\n",
      "0.061 \t\t 0.0956496918777451\n",
      "0.066 \t\t 0.1074114012533735\n",
      "0.071 \t\t 0.12000080706357949\n",
      "0.076 \t\t 0.13341790930836314\n",
      "0.081 \t\t 0.146409555423912\n",
      "0.086 \t\t 0.1595290448740488\n",
      "0.091 \t\t 0.17334534857596345\n",
      "0.096 \t\t 0.18785846652965593\n",
      "0.101 \t\t 0.2030683987351261\n",
      "0.106 \t\t 0.20390790425059155\n",
      "0.111 \t\t 0.20390790425059155\n",
      "0.116 \t\t 0.20390790425059155\n",
      "0.121 \t\t 0.20390790425059155\n",
      "0.126 \t\t 0.20390790425059155\n",
      "0.131 \t\t 0.20390790425059155\n",
      "0.136 \t\t 0.20390790425059155\n",
      "0.141 \t\t 0.20390790425059155\n",
      "0.146 \t\t 0.20390790425059155\n",
      "0.151 \t\t 0.20390790425059155\n",
      "0.156 \t\t 0.20390790425059155\n",
      "0.161 \t\t 0.20390790425059155\n",
      "0.166 \t\t 0.20390790425059155\n",
      "0.171 \t\t 0.20390790425059155\n",
      "0.176 \t\t 0.20390790425059155\n",
      "0.181 \t\t 0.20390790425059155\n",
      "0.186 \t\t 0.20390790425059155\n",
      "0.191 \t\t 0.20390790425059155\n",
      "0.196 \t\t 0.20390790425059155\n",
      "0.201 \t\t 0.20390790425059155\n",
      "0.206 \t\t 0.20390790425059155\n",
      "0.211 \t\t 0.20390790425059155\n",
      "0.216 \t\t 0.20390790425059155\n",
      "0.221 \t\t 0.20390790425059155\n",
      "0.226 \t\t 0.20390790425059155\n",
      "0.231 \t\t 0.20390790425059155\n",
      "0.236 \t\t 0.20390790425059155\n",
      "0.241 \t\t 0.20390790425059155\n",
      "0.246 \t\t 0.20390790425059155\n",
      "0.251 \t\t 0.20390790425059155\n",
      "0.256 \t\t 0.20390790425059155\n",
      "0.261 \t\t 0.20390790425059155\n",
      "0.266 \t\t 0.20390790425059155\n",
      "0.271 \t\t 0.20390790425059155\n",
      "0.276 \t\t 0.20390790425059155\n",
      "0.281 \t\t 0.20390790425059155\n",
      "0.286 \t\t 0.20390790425059155\n",
      "0.291 \t\t 0.20390790425059155\n",
      "0.296 \t\t 0.20390790425059155\n",
      "0.301 \t\t 0.20390790425059155\n",
      "0.306 \t\t 0.20390790425059155\n",
      "0.311 \t\t 0.20390790425059155\n",
      "0.316 \t\t 0.20390790425059155\n",
      "0.321 \t\t 0.20390790425059155\n",
      "0.326 \t\t 0.20390790425059155\n",
      "0.331 \t\t 0.20390790425059155\n",
      "0.336 \t\t 0.20390790425059155\n",
      "0.341 \t\t 0.20390790425059155\n",
      "0.346 \t\t 0.20390790425059155\n",
      "0.351 \t\t 0.20390790425059155\n",
      "0.356 \t\t 0.20390790425059155\n",
      "0.361 \t\t 0.20390790425059155\n",
      "0.366 \t\t 0.20390790425059155\n",
      "0.371 \t\t 0.20390790425059155\n",
      "0.376 \t\t 0.20390790425059155\n",
      "0.381 \t\t 0.20390790425059155\n",
      "0.386 \t\t 0.20390790425059155\n",
      "0.391 \t\t 0.20390790425059155\n",
      "0.396 \t\t 0.20390790425059155\n",
      "0.401 \t\t 0.20390790425059155\n",
      "0.406 \t\t 0.20390790425059155\n",
      "0.411 \t\t 0.20390790425059155\n",
      "0.416 \t\t 0.20390790425059155\n",
      "0.421 \t\t 0.20390790425059155\n",
      "0.426 \t\t 0.20390790425059155\n",
      "0.431 \t\t 0.20390790425059155\n",
      "0.436 \t\t 0.20390790425059155\n",
      "0.441 \t\t 0.20390790425059155\n",
      "0.446 \t\t 0.20390790425059155\n",
      "0.451 \t\t 0.20390790425059155\n",
      "0.456 \t\t 0.20390790425059155\n",
      "0.461 \t\t 0.20390790425059155\n",
      "0.466 \t\t 0.20390790425059155\n",
      "0.471 \t\t 0.20390790425059155\n",
      "0.476 \t\t 0.20390790425059155\n",
      "0.481 \t\t 0.20390790425059155\n",
      "0.486 \t\t 0.20390790425059155\n",
      "0.491 \t\t 0.20390790425059155\n",
      "0.496 \t\t 0.20390790425059155\n",
      "0.501 \t\t 0.20390790425059155\n",
      "0.506 \t\t 0.20390790425059155\n",
      "0.511 \t\t 0.20390790425059155\n",
      "0.516 \t\t 0.20390790425059155\n",
      "0.521 \t\t 0.20390790425059155\n",
      "0.526 \t\t 0.20390790425059155\n",
      "0.531 \t\t 0.20390790425059155\n",
      "0.536 \t\t 0.20390790425059155\n",
      "0.541 \t\t 0.20390790425059155\n",
      "0.546 \t\t 0.20390790425059155\n",
      "0.551 \t\t 0.20390790425059155\n",
      "0.556 \t\t 0.20390790425059155\n",
      "0.561 \t\t 0.20390790425059155\n",
      "0.566 \t\t 0.20390790425059155\n",
      "0.571 \t\t 0.20390790425059155\n",
      "0.576 \t\t 0.20390790425059155\n",
      "0.581 \t\t 0.20390790425059155\n",
      "0.586 \t\t 0.20390790425059155\n",
      "0.591 \t\t 0.20390790425059155\n",
      "0.596 \t\t 0.20390790425059155\n",
      "0.601 \t\t 0.20390790425059155\n",
      "0.606 \t\t 0.20390790425059155\n",
      "0.611 \t\t 0.20390790425059155\n",
      "0.616 \t\t 0.20390790425059155\n",
      "0.621 \t\t 0.20390790425059155\n",
      "0.626 \t\t 0.20390790425059155\n",
      "0.631 \t\t 0.20390790425059155\n",
      "0.636 \t\t 0.20390790425059155\n",
      "0.641 \t\t 0.20390790425059155\n",
      "0.646 \t\t 0.20390790425059155\n",
      "0.651 \t\t 0.20390790425059155\n",
      "0.656 \t\t 0.20390790425059155\n",
      "0.661 \t\t 0.20390790425059155\n",
      "0.666 \t\t 0.20390790425059155\n",
      "0.671 \t\t 0.20390790425059155\n",
      "0.676 \t\t 0.20390790425059155\n",
      "0.681 \t\t 0.20390790425059155\n",
      "0.686 \t\t 0.20390790425059155\n",
      "0.691 \t\t 0.20390790425059155\n",
      "0.696 \t\t 0.20390790425059155\n",
      "0.701 \t\t 0.20390790425059155\n",
      "0.706 \t\t 0.20390790425059155\n",
      "0.711 \t\t 0.20390790425059155\n",
      "0.716 \t\t 0.20390790425059155\n",
      "0.721 \t\t 0.20390790425059155\n",
      "0.726 \t\t 0.20390790425059155\n",
      "0.731 \t\t 0.20390790425059155\n",
      "0.736 \t\t 0.20390790425059155\n",
      "0.741 \t\t 0.20390790425059155\n",
      "0.746 \t\t 0.20390790425059155\n",
      "0.751 \t\t 0.20390790425059155\n",
      "0.756 \t\t 0.20390790425059155\n",
      "0.761 \t\t 0.20390790425059155\n",
      "0.766 \t\t 0.20390790425059155\n",
      "0.771 \t\t 0.20390790425059155\n",
      "0.776 \t\t 0.20390790425059155\n",
      "0.781 \t\t 0.20390790425059155\n",
      "0.786 \t\t 0.20390790425059155\n",
      "0.791 \t\t 0.20390790425059155\n",
      "0.796 \t\t 0.20390790425059155\n",
      "0.801 \t\t 0.20390790425059155\n",
      "0.806 \t\t 0.20390790425059155\n",
      "0.811 \t\t 0.20390790425059155\n",
      "0.816 \t\t 0.20390790425059155\n",
      "0.821 \t\t 0.20390790425059155\n",
      "0.826 \t\t 0.20390790425059155\n",
      "0.831 \t\t 0.20390790425059155\n",
      "0.836 \t\t 0.20390790425059155\n",
      "0.841 \t\t 0.20390790425059155\n",
      "0.846 \t\t 0.20390790425059155\n",
      "0.851 \t\t 0.20390790425059155\n",
      "0.856 \t\t 0.20390790425059155\n",
      "0.861 \t\t 0.20390790425059155\n",
      "0.866 \t\t 0.20390790425059155\n",
      "0.871 \t\t 0.20390790425059155\n",
      "0.876 \t\t 0.20390790425059155\n",
      "0.881 \t\t 0.20390790425059155\n",
      "0.886 \t\t 0.20390790425059155\n",
      "0.891 \t\t 0.20390790425059155\n",
      "0.896 \t\t 0.20390790425059155\n",
      "0.901 \t\t 0.20390790425059155\n",
      "0.906 \t\t 0.20390790425059155\n",
      "0.911 \t\t 0.20390790425059155\n",
      "0.916 \t\t 0.20390790425059155\n",
      "0.921 \t\t 0.20390790425059155\n",
      "0.926 \t\t 0.20390790425059155\n",
      "0.931 \t\t 0.20390790425059155\n",
      "0.936 \t\t 0.20390790425059155\n",
      "0.941 \t\t 0.20390790425059155\n",
      "0.946 \t\t 0.20390790425059155\n",
      "0.951 \t\t 0.20390790425059155\n",
      "0.956 \t\t 0.20390790425059155\n",
      "0.961 \t\t 0.20390790425059155\n",
      "0.966 \t\t 0.20390790425059155\n",
      "0.971 \t\t 0.20390790425059155\n",
      "0.976 \t\t 0.20390790425059155\n",
      "0.981 \t\t 0.20390790425059155\n",
      "0.986 \t\t 0.20390790425059155\n",
      "0.991 \t\t 0.20390790425059155\n",
      "0.996 \t\t 0.20390790425059155\n"
     ]
    }
   ],
   "source": [
    "# implement Lasso regression and make a table where you explore the effect of different values of `alpha`\n",
    "from sklearn import linear_model\n",
    "\n",
    "alpha=[]\n",
    "curr=0.001\n",
    "while curr<1:\n",
    "    alpha.append(curr)\n",
    "    curr+=0.005\n",
    "\n",
    "n_samples=len(x_train)\n",
    "n_features = len(x_train[0])\n",
    "errorForAlpha=[]\n",
    "\n",
    "for lrate in alpha:\n",
    "    \n",
    "    clf = linear_model.Lasso(alpha=lrate)\n",
    "    clf.fit(x_train,y_train)\n",
    "    output=clf.predict(x_test)\n",
    "    \n",
    "    sqaurError=0\n",
    "    for i in range(len(output)):\n",
    "        sqaurError+=(output[i]-y_test[i])**2\n",
    "    sqaurError/=len(output)\n",
    "    \n",
    "    errorForAlpha.append(sqaurError)\n",
    "\n",
    "print(\"Learning Rate\\tAverage Squared Error\")\n",
    "for i in range(len(alpha)):\n",
    "    print(round(alpha[i],3),\"\\t\\t\",errorForAlpha[i])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
